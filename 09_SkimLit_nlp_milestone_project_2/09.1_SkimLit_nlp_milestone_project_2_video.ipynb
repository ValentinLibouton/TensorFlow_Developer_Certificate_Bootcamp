{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Milestone Project 2: SkimLit ðŸ“„ðŸ”¥\n",
    "\n",
    "The purpose of this notebook is to build an NLP model to make reading medical abstracts easier.\n",
    "\n",
    "The paper we're replicating (the source of the dataset that we'll be using) is available here: [https://arxiv.org/abs/1710.06071](https://arxiv.org/abs/1710.06071)\n",
    "\n",
    "And reading through the paper above, we see that the model architecture that they use to achieve their best results is available here: [https://arxiv.org/abs/1612.05251](https://arxiv.org/abs/1612.05251)\n",
    "\n",
    "ðŸ“– **Resource:** If you want to find the ground truth for this notebook (with lots of diagrams and text annotations) see the GitHub: [https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/09_SkimLit_nlp_milestone_project_2.ipynb](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/09_SkimLit_nlp_milestone_project_2.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de86b8ef3e9ed643"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°259: Setting up our notebook for Milestone Project 2 (getting the data) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64643ca88452661"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check GPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f4272e18e537d3"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:49.979736094Z",
     "start_time": "2024-05-03T19:51:44.895914509Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 21:51:45.425588: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-03 21:51:45.425622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-03 21:51:45.449859: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-03 21:51:45.502394: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 21:51:49.523975: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-03 21:51:49.823764: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-03 21:51:49.824050: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-03 21:51:49.837042: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-03 21:51:49.837352: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-03 21:51:49.837595: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-03 21:51:49.937429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-03 21:51:49.937768: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-03 21:51:49.937999: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-03 21:51:49.938188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5581 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from MachineLearningUtils.system_setup.gpu import activate_gpu\n",
    "activate_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "GPU 0: NVIDIA GeForce RTX 3050 (UUID: GPU-ee6cd8ff-34bf-dc58-2edb-4f5120aaec55)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:50.247420350Z",
     "start_time": "2024-05-03T19:51:49.970843977Z"
    }
   },
   "id": "df85096a995054bf",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9027ad4f6a52cc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from MachineLearningUtils.data_acquisition.data_downloader import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:50.295626988Z",
     "start_time": "2024-05-03T19:51:50.249852263Z"
    }
   },
   "id": "365366cb92659f4f",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get data\n",
    "\n",
    "Since we'll be replicating the paper above (PubMed 200k RCT), let's download the dataset they used.\n",
    "We can do so from the authors GitHub: [https://github.com/Franck-Dernoncourt/pubmed-rct](https://github.com/Franck-Dernoncourt/pubmed-rct)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d1583e7355de7b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "fatal: destination path 'pubmed-rct' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:50.506791500Z",
     "start_time": "2024-05-03T19:51:50.272995842Z"
    }
   },
   "id": "7d05453662291490",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "PubMed_200k_RCT\r\n",
      "PubMed_200k_RCT_numbers_replaced_with_at_sign\r\n",
      "PubMed_20k_RCT\r\n",
      "PubMed_20k_RCT_numbers_replaced_with_at_sign\r\n",
      "README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls pubmed-rct"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:50.734530864Z",
     "start_time": "2024-05-03T19:51:50.509128066Z"
    }
   },
   "id": "15572a1eed16093a",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "dev.txt  test.txt  train.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Check what files are in the PubMed_20K dataset\n",
    "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:50.973846480Z",
     "start_time": "2024-05-03T19:51:50.737853384Z"
    }
   },
   "id": "a9a40392efde889f",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Start our experiments using the 20k dataset with numbers replaced by \"@\" sign\n",
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.021087217Z",
     "start_time": "2024-05-03T19:51:50.978297589Z"
    }
   },
   "id": "b4b2c92ef1cec889",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all of the filenames in the target directory\n",
    "import os\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.042546471Z",
     "start_time": "2024-05-03T19:51:51.021136729Z"
    }
   },
   "id": "4aad2f33e3c1e1fa",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°260: Visualising examples from the dataset (becoming one with the data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f7914964f35401b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess data\n",
    "\n",
    "Now we've got some text data, it's time to become one with it.\n",
    "\n",
    "And one of the best ways to become one with the data is to...\n",
    "\n",
    "> Visualize, visualize, visualize \n",
    "\n",
    "So with that in mind, let's write a function to read in all of the lines of a target text file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed02247c0c97fa6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "  \"\"\"\n",
    "  Reads filename (a text filename) and returns the lines of text as a list.\n",
    "\n",
    "  Args:\n",
    "    filename: a string containing the target filepath.\n",
    "\n",
    "  Returns:\n",
    "    A list of strings with one string per line from the target filename.\n",
    "  \"\"\"\n",
    "  with open(filename, \"r\") as f:\n",
    "    return f.readlines()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.046252956Z",
     "start_time": "2024-05-03T19:51:51.021774817Z"
    }
   },
   "id": "1c5a79afeee2449a",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['###24293578\\n',\n 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n '\\n',\n '###24854809\\n',\n 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n',\n 'METHODS\\tAttentional biases for high caloric foods were measured by eye tracking during a visual probe task with pictorial food and neutral stimuli .\\n',\n 'METHODS\\tSelf-reported emotional eating was assessed with the Dutch Eating Behavior Questionnaire ( DEBQ ) and ad libitum food intake was tested by a disguised food offer .\\n',\n 'RESULTS\\tHierarchical multivariate regression modeling showed that self-reported emotional eating did not account for changes in attention allocation for food or food intake in either condition .\\n',\n 'RESULTS\\tYet , attention maintenance on food cues was significantly related to increased intake specifically in the neutral condition , but not in the sad mood condition .\\n',\n 'CONCLUSIONS\\tThe current findings show that self-reported emotional eating ( based on the DEBQ ) might not validly predict who overeats when sad , at least not in a laboratory setting with healthy women .\\n',\n 'CONCLUSIONS\\tResults further suggest that attention maintenance on food relates to eating motivation when in a neutral affective state , and might therefore be a cognitive mechanism contributing to increased food intake in general , but maybe not during sad mood .\\n',\n '\\n']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in the training lines\n",
    "train_lines = get_lines(data_dir+\"train.txt\") # read the lines with the training file\n",
    "train_lines[:27]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.154606460Z",
     "start_time": "2024-05-03T19:51:51.022206209Z"
    }
   },
   "id": "c73a9b75b52c3297",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "210040"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.159173426Z",
     "start_time": "2024-05-03T19:51:51.067738128Z"
    }
   },
   "id": "914a77dd14050b89",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's think about how we want our data to look...\n",
    "\n",
    "How I think our data would be best represented...\n",
    "\n",
    "```\n",
    "[{'line_number': 0,\n",
    "   'target': 'BACKGROUND',\n",
    "   'text': \"Emotional eating is associated with overeating and the development of obesity .\\n\"\n",
    "   'total_lines': 11},\n",
    "   ...]\n",
    "```\n",
    "\n",
    "Let's write a function which turns each of our datasets into the above format so we can continue to prepare our data for modelling."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce524bdb3f99abca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°261: writing a preprocessing function to structure our data for modelling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c01f938fe69421"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "  \"\"\"\n",
    "  Returns a list of dictionaries of abstract line data.\n",
    "  \n",
    "  Takes in filename, reads it contents and sorts through each line,\n",
    "  extracting things like the target label, the text of the sentnece,\n",
    "  how many sentences are in the current abstract and what sentence\n",
    "  number the target line is.\n",
    "  \"\"\"\n",
    "  input_lines = get_lines(filename=filename) # get all lines from filename\n",
    "  abstract_lines = \"\" # create an empty abstract\n",
    "  abstract_samples = [] # create an empty list of abstracts\n",
    "  \n",
    "  # Loop through each line in the target file\n",
    "  for line in input_lines:\n",
    "    if line.startswith(\"###\"): # check to see if the is an ID line\n",
    "      abstract_id = line\n",
    "      abstract_lines = \"\" # reset the abstract string if the line is an ID line\n",
    "    elif line.isspace():\n",
    "      abstract_line_split = abstract_lines.splitlines()\n",
    "    \n",
    "      # Iterate through each line in a single abstract and count them at the same time\n",
    "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "        line_data = {}\n",
    "        target_text_split = abstract_line.split(\"\\t\")\n",
    "        line_data[\"target\"] = target_text_split[0]\n",
    "        line_data[\"text\"] = target_text_split[1].lower()\n",
    "        line_data[\"line_number\"] = abstract_line_number\n",
    "        line_data[\"total_lines\"] = len(abstract_line_split) - 1\n",
    "        abstract_samples.append(line_data)\n",
    "    else:\n",
    "      abstract_lines += line\n",
    "  return abstract_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.175557478Z",
     "start_time": "2024-05-03T19:51:51.081660239Z"
    }
   },
   "id": "5dd36e95d010b5bb",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180040 30212 30135\n",
      "CPU times: user 439 ms, sys: 60.1 ms, total: 499 ms\n",
      "Wall time: 496 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get data from file and preprocess it\n",
    "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation dataset\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
    "print(len(train_samples), len(val_samples), len(test_samples))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.592859407Z",
     "start_time": "2024-05-03T19:51:51.086972693Z"
    }
   },
   "id": "95f26491dd2e6852",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[{'target': 'OBJECTIVE',\n  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n  'line_number': 0,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n  'line_number': 1,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n  'line_number': 2,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n  'line_number': 3,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n  'line_number': 4,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n  'line_number': 5,\n  'total_lines': 11},\n {'target': 'RESULTS',\n  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n  'line_number': 6,\n  'total_lines': 11},\n {'target': 'RESULTS',\n  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n  'line_number': 7,\n  'total_lines': 11}]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first abstract of our training data\n",
    "train_samples[:8]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.604944290Z",
     "start_time": "2024-05-03T19:51:51.588237238Z"
    }
   },
   "id": "efebe0517d07729d",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°262: Performing visual data analysis on our preprocessed text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e76156ef76f753d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that our data is the format of a list of dictionaries, how about we trun it into a DataFrame to further visualize it?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1badc574ddf41881"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         target                                               text  \\\n0     OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n1       METHODS  a total of @ patients with primary knee oa wer...   \n2       METHODS  outcome measures included pain reduction and i...   \n3       METHODS  pain was assessed using the visual analog pain...   \n4       METHODS  secondary outcome measures included the wester...   \n5       METHODS  serum levels of interleukin @ ( il-@ ) , il-@ ...   \n6       RESULTS  there was a clinically relevant reduction in t...   \n7       RESULTS  the mean difference between treatment arms ( @...   \n8       RESULTS  further , there was a clinically relevant redu...   \n9       RESULTS  these differences remained significant at @ we...   \n10      RESULTS  the outcome measures in rheumatology clinical ...   \n11  CONCLUSIONS  low-dose oral prednisolone had both a short-te...   \n12   BACKGROUND  emotional eating is associated with overeating...   \n13   BACKGROUND  yet , empirical evidence for individual ( trai...   \n14    OBJECTIVE  the aim of this study was to test if attention...   \n15    OBJECTIVE  it was expected that emotional eating is predi...   \n16      METHODS  participants ( n = @ ) were randomly assigned ...   \n17      METHODS  attentional biases for high caloric foods were...   \n18      METHODS  self-reported emotional eating was assessed wi...   \n19      RESULTS  hierarchical multivariate regression modeling ...   \n\n    line_number  total_lines  \n0             0           11  \n1             1           11  \n2             2           11  \n3             3           11  \n4             4           11  \n5             5           11  \n6             6           11  \n7             7           11  \n8             8           11  \n9             9           11  \n10           10           11  \n11           11           11  \n12            0           10  \n13            1           10  \n14            2           10  \n15            3           10  \n16            4           10  \n17            5           10  \n18            6           10  \n19            7           10  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n      <th>line_number</th>\n      <th>total_lines</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>OBJECTIVE</td>\n      <td>to investigate the efficacy of @ weeks of dail...</td>\n      <td>0</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>METHODS</td>\n      <td>a total of @ patients with primary knee oa wer...</td>\n      <td>1</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>METHODS</td>\n      <td>outcome measures included pain reduction and i...</td>\n      <td>2</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>METHODS</td>\n      <td>pain was assessed using the visual analog pain...</td>\n      <td>3</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>METHODS</td>\n      <td>secondary outcome measures included the wester...</td>\n      <td>4</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>METHODS</td>\n      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n      <td>5</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>RESULTS</td>\n      <td>there was a clinically relevant reduction in t...</td>\n      <td>6</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>RESULTS</td>\n      <td>the mean difference between treatment arms ( @...</td>\n      <td>7</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>RESULTS</td>\n      <td>further , there was a clinically relevant redu...</td>\n      <td>8</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>RESULTS</td>\n      <td>these differences remained significant at @ we...</td>\n      <td>9</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>RESULTS</td>\n      <td>the outcome measures in rheumatology clinical ...</td>\n      <td>10</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>CONCLUSIONS</td>\n      <td>low-dose oral prednisolone had both a short-te...</td>\n      <td>11</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>BACKGROUND</td>\n      <td>emotional eating is associated with overeating...</td>\n      <td>0</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>BACKGROUND</td>\n      <td>yet , empirical evidence for individual ( trai...</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>OBJECTIVE</td>\n      <td>the aim of this study was to test if attention...</td>\n      <td>2</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>OBJECTIVE</td>\n      <td>it was expected that emotional eating is predi...</td>\n      <td>3</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>METHODS</td>\n      <td>participants ( n = @ ) were randomly assigned ...</td>\n      <td>4</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>METHODS</td>\n      <td>attentional biases for high caloric foods were...</td>\n      <td>5</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>METHODS</td>\n      <td>self-reported emotional eating was assessed wi...</td>\n      <td>6</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>RESULTS</td>\n      <td>hierarchical multivariate regression modeling ...</td>\n      <td>7</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.809695624Z",
     "start_time": "2024-05-03T19:51:51.624488720Z"
    }
   },
   "id": "b9dd612854ee00d8",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "target\nMETHODS        59353\nRESULTS        57953\nCONCLUSIONS    27168\nBACKGROUND     21727\nOBJECTIVE      13839\nName: count, dtype: int64"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of labels in training data\n",
    "train_df.target.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:51.847323628Z",
     "start_time": "2024-05-03T19:51:51.820948890Z"
    }
   },
   "id": "412ce5e85978a236",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check the length of different lines\n",
    "train_df.total_lines.plot.hist(); "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:52.309635877Z",
     "start_time": "2024-05-03T19:51:51.825683579Z"
    }
   },
   "id": "fb875b20e23c11de",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get lists of sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "644c18ee45459b27"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(180040, 30212, 30135)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert abstract text lines into lists\n",
    "train_sentences = train_df[\"text\"].to_list()\n",
    "val_sentences = val_df[\"text\"].to_list()\n",
    "test_sentences = test_df[\"text\"].to_list()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:52.352264238Z",
     "start_time": "2024-05-03T19:51:52.312983416Z"
    }
   },
   "id": "32e815ce86e2cae4",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n 'these differences remained significant at @ weeks .']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the 10 lines of training sentences\n",
    "train_sentences[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:52.353334279Z",
     "start_time": "2024-05-03T19:51:52.321915126Z"
    }
   },
   "id": "6b1a6c5af204ed18",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°263: Turning our target labels into numbers (ML models require numbers)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e253c33607328376"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make numeric labels (ML models require numeric labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d37c9b9c38abf423"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 0.]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encode labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False) # we want non-sparse matrix\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# check what one hot encoded labels look like\n",
    "train_labels_one_hot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:52.761744422Z",
     "start_time": "2024-05-03T19:51:52.329039593Z"
    }
   },
   "id": "97dea573f5a3adde",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Label encode labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3b2fa5389c5db4e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([3, 2, 2, ..., 4, 1, 1])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract labels (\"target\" columns) and encode them into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "test_label_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "# check what training labels look like\n",
    "train_labels_encoded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:52.841876819Z",
     "start_time": "2024-05-03T19:51:52.757352790Z"
    }
   },
   "id": "de91de4cfe9de554",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(5,\n array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n       dtype=object))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names and number of classes from LabelEncoder instance\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:52.845872865Z",
     "start_time": "2024-05-03T19:51:52.802872131Z"
    }
   },
   "id": "d621a65ce0ca57b1",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°264: Model 0: Creating, fitting and evaluating a baseline model for SkimLit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fddf374866019691"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Starting a series of modelling experiments...\n",
    "\n",
    "As usual, we're going to be trying out a bunch of different models and seeing which one works best.\n",
    "\n",
    "And as always, we're going to start with a baseline (TF-IDF Multinomial Naive Bayes classifier)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baa8905424ba279f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 0: Getting a baseline\n",
    "\n",
    "A baseline model is an important first step in any series of modelling experiments.\n",
    "\n",
    "It sets the pace for the subsequent experiments.\n",
    "\n",
    "It should start with a simple model and later models (with increasing complexity) will try to beat it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a215e3afaa1d3af7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())])",
      "text/html": "<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"â–¸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"â–¾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tf-idf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;tf-idf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div></div></div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "model_0 = Pipeline([\n",
    "  ('tf-idf', TfidfVectorizer()),\n",
    "  ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_sentences,\n",
    "            y=train_labels_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:56.826419170Z",
     "start_time": "2024-05-03T19:51:52.807588326Z"
    }
   },
   "id": "bbd00c93d89e0cc1",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.7218323844829869"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate baseline model on validation dataset\n",
    "model_0.score(X=val_sentences, y=val_labels_encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:57.329751855Z",
     "start_time": "2024-05-03T19:51:56.862916628Z"
    }
   },
   "id": "cd7e196f61b1ef8b",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([4, 1, 3, ..., 4, 4, 1])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using our baseline model\n",
    "baseline_preds = model_0.predict(X=val_sentences)\n",
    "baseline_preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:57.774539755Z",
     "start_time": "2024-05-03T19:51:57.399341411Z"
    }
   },
   "id": "c32b0e0361ace420",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from MachineLearningUtils.data_visualization.evaluation_metrics import evaluate_classification_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:57.792497588Z",
     "start_time": "2024-05-03T19:51:57.769934940Z"
    }
   },
   "id": "a4b38cc27e0ab6b7",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 72.1832384482987,\n 'precision': 0.7186466952323352,\n 'recall': 0.7218323844829869,\n 'f1': 0.6989250353450294}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate beseline results\n",
    "baseline_results = evaluate_classification_metrics(y_true=val_labels_encoded,\n",
    "                                                   y_pred=baseline_preds)\n",
    "baseline_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:57.815380131Z",
     "start_time": "2024-05-03T19:51:57.775288118Z"
    }
   },
   "id": "21ae8f2f510b8363",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°265: Preparing our data for deep sequence models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4df6a1086953b65a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing our data (the text) for deep sequence models\n",
    "\n",
    "Before we start building deeper models, we've got to create vectorization and embedding layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d42cf2aab2e6d9d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:57.912270518Z",
     "start_time": "2024-05-03T19:51:57.793550909Z"
    }
   },
   "id": "8a98148acbd53d97",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n 'these differences remained significant at @ weeks .']"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:57.938470069Z",
     "start_time": "2024-05-03T19:51:57.819862528Z"
    }
   },
   "id": "6e0d038edfe8db3f",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "26.338269273494777"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long is each sentence on average\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_lens)\n",
    "avg_sent_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:58.006509549Z",
     "start_time": "2024-05-03T19:51:57.861142915Z"
    }
   },
   "id": "9f031e4e5a3647a3",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(array([4.2075e+04, 8.3771e+04, 3.6877e+04, 1.0945e+04, 3.9310e+03,\n        1.4450e+03, 5.6000e+02, 2.2600e+02, 1.0100e+02, 4.5000e+01,\n        2.0000e+01, 1.2000e+01, 9.0000e+00, 1.0000e+01, 6.0000e+00,\n        2.0000e+00, 3.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00]),\n array([  1.  ,  15.75,  30.5 ,  45.25,  60.  ,  74.75,  89.5 , 104.25,\n        119.  , 133.75, 148.5 , 163.25, 178.  , 192.75, 207.5 , 222.25,\n        237.  , 251.75, 266.5 , 281.25, 296.  ]),\n <BarContainer object of 20 artists>)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What's the distribution look like?\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sent_lens, bins=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:58.482809634Z",
     "start_time": "2024-05-03T19:51:57.998870401Z"
    }
   },
   "id": "eb6cab522b5f18b3",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "55"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How long of a sentence lenght covers 95% of examples?\n",
    "output_seq_len = int(np.percentile(sent_lens, 95))\n",
    "output_seq_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:58.514398150Z",
     "start_time": "2024-05-03T19:51:58.485858513Z"
    }
   },
   "id": "e191fc3a0943fcb0",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "296"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum sequence length in the training set\n",
    "max(sent_lens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:58.605082625Z",
     "start_time": "2024-05-03T19:51:58.494829793Z"
    }
   },
   "id": "f2fab5acee93561d",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°266: Creating a text vectoriser to map our tokens (text) to numbers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3893f9ed26d4342b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create text vectorizer layer\n",
    "\n",
    "We want to make a layer which maps our texts from words to numbers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a4112b57a484988"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# How many words are in our vocab? (taken from table 2 in: https://arxiv.org/pdf/1710.06071.pdf\n",
    "max_tokens = 68000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:58.608032880Z",
     "start_time": "2024-05-03T19:51:58.537419097Z"
    }
   },
   "id": "29dfad0cbb911e4c",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create text vectorizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens, # number of words in vocabulary\n",
    "                                    output_sequence_length=output_seq_len) # desired output length of vectorized sequences\n",
    "                                    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:51:58.661606143Z",
     "start_time": "2024-05-03T19:51:58.538109373Z"
    }
   },
   "id": "2d18b882f2877bc9",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adapt text vectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:06.531199495Z",
     "start_time": "2024-05-03T19:51:58.572084534Z"
    }
   },
   "id": "19657f08b87b54c2",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "hispanic women ( n = @ ) with stage @ to iii breast cancer who completed adjuvant treatment and lived in new york city were randomized between april @ and march @ .\n",
      "\n",
      "Lenght of text: 33\n",
      "\n",
      "Vectorized text:\n",
      "[[4200   90   40    7  531    6  554  413  135   67  253 1071   19    3\n",
      "  8892    5  319 3493 3420    9   29   30 1441    3 1247    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# Test out text vectorizer on random sentences\n",
    "import random\n",
    "target_sentence = random.choice(train_sentences)\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLenght of text: {len(target_sentence.split())}\")\n",
    "print(f\"\\nVectorized text:\\n{text_vectorizer([target_sentence])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:07.309729088Z",
     "start_time": "2024-05-03T19:52:06.533451153Z"
    }
   },
   "id": "bb3f27966e4cc3c6",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 64841\n",
      "Most common words in the vocab: ['', '[UNK]', 'the', 'and', 'of']\n",
      "Least common words in the vocab: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "# How many words in our training vocabulary\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocab: {len(rct_20k_text_vocab)}\")\n",
    "print(f\"Most common words in the vocab: {rct_20k_text_vocab[:5]}\")\n",
    "print(f\"Least common words in the vocab: {rct_20k_text_vocab[-5:]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:07.415815125Z",
     "start_time": "2024-05-03T19:52:07.304701053Z"
    }
   },
   "id": "1315ec9b88881b47",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'name': 'text_vectorization',\n 'trainable': True,\n 'dtype': 'string',\n 'batch_input_shape': (None,),\n 'max_tokens': 68000,\n 'standardize': 'lower_and_strip_punctuation',\n 'split': 'whitespace',\n 'ngrams': None,\n 'output_mode': 'int',\n 'output_sequence_length': 55,\n 'pad_to_max_tokens': False,\n 'sparse': False,\n 'ragged': False,\n 'vocabulary': None,\n 'idf_weights': None,\n 'encoding': 'utf-8',\n 'vocabulary_size': 64841}"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the config of our text vectorizer\n",
    "text_vectorizer.get_config()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:07.431357016Z",
     "start_time": "2024-05-03T19:52:07.414583338Z"
    }
   },
   "id": "96dd730cb3cf01b",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°267: Creating a custom token embedding layer with TensorFlow"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9adf171d258cf255"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create custom text embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "540e6261e85b27a2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create token embedding layer\n",
    "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocabulary\n",
    "                               output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train)\n",
    "                               mask_zero=True, # use masking to handle variable sequence lengths (save space)\n",
    "                               name=\"token_embedding\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:07.435661172Z",
     "start_time": "2024-05-03T19:52:07.423404317Z"
    }
   },
   "id": "6dc4ebc5fa11b387",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence before vectorization:\n",
      " hispanic women ( n = @ ) with stage @ to iii breast cancer who completed adjuvant treatment and lived in new york city were randomized between april @ and march @ .\n",
      "\n",
      "Sentence after vectorization (before embedding):\n",
      " [[4200   90   40    7  531    6  554  413  135   67  253 1071   19    3\n",
      "  8892    5  319 3493 3420    9   29   30 1441    3 1247    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Sentence after embedding:\n",
      " [[[-0.0389842   0.04694922 -0.02654424 ...  0.01682257  0.01301061\n",
      "    0.04130484]\n",
      "  [ 0.02958651  0.02730762  0.01181011 ...  0.00823455 -0.01152806\n",
      "    0.04408984]\n",
      "  [ 0.04983633  0.01609386 -0.02099669 ... -0.01481913 -0.00371032\n",
      "   -0.02961998]\n",
      "  ...\n",
      "  [ 0.02564664 -0.03405463  0.03228176 ... -0.04206408 -0.00787473\n",
      "   -0.00700234]\n",
      "  [ 0.02564664 -0.03405463  0.03228176 ... -0.04206408 -0.00787473\n",
      "   -0.00700234]\n",
      "  [ 0.02564664 -0.03405463  0.03228176 ... -0.04206408 -0.00787473\n",
      "   -0.00700234]]]\n",
      "Embedded sentence shape: (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "# Show example embedding\n",
    "print(f\"Sentence before vectorization:\\n {target_sentence}\\n\")\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Sentence after vectorization (before embedding):\\n {vectorized_sentence}\")\n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"Sentence after embedding:\\n {embedded_sentence}\")\n",
    "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:07.524558207Z",
     "start_time": "2024-05-03T19:52:07.427440633Z"
    }
   },
   "id": "e665c2f1e25300ac",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°268: Creating fast loading dataset with the TensorFlow tf.data.API "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "802b008a76192a23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating datasets (making sure our data loads as fast as possible)\n",
    "\n",
    "We're going to setup our data to run as fast as possible with the TensorFlow `tf.data` API, many of the steps here are discussed at length in these two resources:\n",
    "* https://www.tensorflow.org/guide/data_performance\n",
    "* https://www.tensorflow.org/guide/data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe396875bd3569b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:07.938278504Z",
     "start_time": "2024-05-03T19:52:07.479469561Z"
    }
   },
   "id": "485751f03d4bef05",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the TensorSliceDataset's and turn them into prefetched datasets\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:07.955166516Z",
     "start_time": "2024-05-03T19:52:07.938898290Z"
    }
   },
   "id": "da5f5b0ee48f0c75",
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°269: Model 1: Building, fitting and evaluating a Conv1D with token embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5838a1a85e8cbc0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 1: Conv1D with token embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0b382dcc9461955"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create 1D conv model to process sequences\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
    "token_embeddings = token_embed(text_vectors) # create embedding\n",
    "x = layers.Conv1D(filters=64,\n",
    "                  kernel_size=5,\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\")(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector from conv layer\n",
    "outputs = layers.Dense(units=num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:08.062379035Z",
     "start_time": "2024-05-03T19:52:07.959724465Z"
    }
   },
   "id": "ef268fec6c377b87",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVe  (None, 55)                0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " token_embedding (Embedding  (None, 55, 128)           8299648   \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 64)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8340997 (31.82 MB)\n",
      "Trainable params: 8340997 (31.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:08.080756727Z",
     "start_time": "2024-05-03T19:52:08.027676459Z"
    }
   },
   "id": "3b7f01aebd715cf7",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 21:52:08.651829: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-05-03 21:52:09.192838: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f505804ef10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-03 21:52:09.192880: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050, Compute Capability 8.6\n",
      "2024-05-03 21:52:09.217087: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714765929.320805    4941 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/562 [==============================] - 18s 29ms/step - loss: 0.9152 - accuracy: 0.6364 - val_loss: 0.6836 - val_accuracy: 0.7390\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 6s 10ms/step - loss: 0.6587 - accuracy: 0.7549 - val_loss: 0.6267 - val_accuracy: 0.7706\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 4s 8ms/step - loss: 0.6173 - accuracy: 0.7734 - val_loss: 0.5959 - val_accuracy: 0.7866\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history_model_1 = model_1.fit(x=train_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=valid_dataset,\n",
    "                              validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:36.475640116Z",
     "start_time": "2024-05-03T19:52:08.069650034Z"
    }
   },
   "id": "74bc51aa2dd4f7b0",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 2s 3ms/step - loss: 0.5978 - accuracy: 0.7862\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.5977728962898254, 0.7861776947975159]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on whole validation dataset\n",
    "model_1.evaluate(x=valid_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:38.970116227Z",
     "start_time": "2024-05-03T19:52:36.475865276Z"
    }
   },
   "id": "2fe88bdb35b65a11",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([[4.34270859e-01, 1.75561562e-01, 8.93370733e-02, 2.67806083e-01,\n         3.30245048e-02],\n        [4.79570299e-01, 2.43155792e-01, 1.42434845e-02, 2.54696459e-01,\n         8.33399594e-03],\n        [1.67084664e-01, 6.50614128e-03, 1.89283991e-03, 8.24468493e-01,\n         4.79059745e-05],\n        ...,\n        [4.73624459e-06, 7.25519145e-04, 8.53071862e-04, 2.08011852e-06,\n         9.98414636e-01],\n        [5.41144460e-02, 4.47078556e-01, 9.93525237e-02, 6.40073121e-02,\n         3.35447192e-01],\n        [1.69363648e-01, 6.91872120e-01, 4.34912704e-02, 3.99100631e-02,\n         5.53628057e-02]], dtype=float32),\n (30212, 5))"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (our model predicts prediction probabilities for each class)\n",
    "model_1_pred_probs = model_1.predict(valid_dataset)\n",
    "model_1_pred_probs, model_1_pred_probs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:41.310016181Z",
     "start_time": "2024-05-03T19:52:38.973010866Z"
    }
   },
   "id": "4ef68cc0b8511a63",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1])>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pred probs to classes\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:41.328543538Z",
     "start_time": "2024-05-03T19:52:41.308598521Z"
    }
   },
   "id": "e6896aa570e494f",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 78.61776777439428,\n 'precision': 0.782444331833818,\n 'recall': 0.7861776777439428,\n 'f1': 0.7833561166041626}"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_1 results\n",
    "model_1_results = evaluate_classification_metrics(y_true=val_labels_encoded,\n",
    "                                                  y_pred=model_1_preds)\n",
    "model_1_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:41.408118949Z",
     "start_time": "2024-05-03T19:52:41.314151369Z"
    }
   },
   "id": "2397edbe39bb6bbc",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([ True,  True,  True,  True])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:52:41.426884630Z",
     "start_time": "2024-05-03T19:52:41.329659954Z"
    }
   },
   "id": "461c09a6fdf69a9a",
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "**model_1** est meilleur en tout point que **baseline**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56095c6eb4195128"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°270: Preparing a pretrained embedding layer from TensorFlow Hub for Model 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be23043113119a97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 2: Feature extraction with pretrained token embeddings\n",
    "\n",
    "Now lets use pretrained word embeddings from TensorFlow Hub, more specifically the universal sentence encoder (USE): https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "\n",
    "The paper originally used GloVe embeddings, however, we're going to stick with the later created USE pretrained embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62adbf98e50892f4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Download pretrained TensorFlow Hub USE\n",
    "import tensorflow_hub as hub\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:53:50.650684076Z",
     "start_time": "2024-05-03T19:52:41.333572486Z"
    }
   },
   "id": "8d2ce5be9bcbb6c6",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sentence:\n",
      " relapsed multiple myeloma has no standard treatment , and the role of autologous stem-cell transplantation ( asct ) has not been fully defined .\n",
      "Setence after embedding:\n",
      "[-0.06423067 -0.08098671 -0.04497732 -0.00451922 -0.03398822 -0.06659217\n",
      "  0.05415363 -0.0020579   0.07296965  0.03477467  0.09305882  0.03807441\n",
      " -0.0325536   0.05058302  0.05226684 -0.00821479 -0.0950499   0.06015294\n",
      " -0.07544212  0.05744001  0.01902379  0.05058502  0.00594179 -0.01076407\n",
      "  0.06608254 -0.04028416  0.03065692  0.00173584  0.00136049  0.05535701]\n",
      "\n",
      "Length of sentence embedding: 512\n"
     ]
    }
   ],
   "source": [
    "# Test out the pretrained embedding on a random sentence\n",
    "random_train_sentence = random.choice(train_sentences)\n",
    "print(f\"Random sentence:\\n {random_train_sentence}\")\n",
    "use_embedded_sentence = tf_hub_embedding_layer([random_train_sentence])\n",
    "print(f\"Setence after embedding:\\n{use_embedded_sentence[0][:30]}\\n\")\n",
    "print(f\"Length of sentence embedding: {len(use_embedded_sentence[0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:53:52.541747124Z",
     "start_time": "2024-05-03T19:53:50.645806521Z"
    }
   },
   "id": "d0c58a88b7158cc6",
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°271: Model 2:Building, fitting and evaluating a Conv1D model with token embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "894938efb8263914"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building and fitting an NLP feature extraction model using pretrained embeddings TensorFlow Hub"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c98cc3cac193b364"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define feature extraction model using TF Hub layer\n",
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "pretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding of each sequence (512 long vector)\n",
    "x = layers.Dense(units=128, activation=\"relu\")(pretrained_embedding)\n",
    "# Note: you could add more layers here if you wanted to\n",
    "outputs = layers.Dense(units=num_classes, activation=\"softmax\")(x)\n",
    "model_2 = tf.keras.Model(inputs=inputs,\n",
    "                         outputs=outputs,\n",
    "                         name=\"model_2_USE_feature_extractor\")\n",
    "\n",
    "# Compile the model\n",
    "model_2.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:53:52.960955797Z",
     "start_time": "2024-05-03T19:53:52.538320363Z"
    }
   },
   "id": "da6240508e59b524",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_USE_feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " universal_sentence_encoder  (None, 512)               256797824 \n",
      "  (KerasLayer)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256864133 (979.86 MB)\n",
      "Trainable params: 66309 (259.02 KB)\n",
      "Non-trainable params: 256797824 (979.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:53:53.075030428Z",
     "start_time": "2024-05-03T19:53:52.961911575Z"
    }
   },
   "id": "e987bbd026ef65c7",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 11s 15ms/step - loss: 0.9177 - accuracy: 0.6515 - val_loss: 0.7969 - val_accuracy: 0.6912\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 7s 12ms/step - loss: 0.7696 - accuracy: 0.7027 - val_loss: 0.7554 - val_accuracy: 0.7045\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 8s 14ms/step - loss: 0.7533 - accuracy: 0.7115 - val_loss: 0.7395 - val_accuracy: 0.7151\n"
     ]
    }
   ],
   "source": [
    "# Fit model_2 to the data\n",
    "history_model_2 = model_2.fit(x=train_dataset,\n",
    "                              epochs=3,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "                              validation_data=valid_dataset,\n",
    "                              validation_steps=int(0.1 * len(valid_dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:18.334507672Z",
     "start_time": "2024-05-03T19:53:53.024678225Z"
    }
   },
   "id": "1faff0370aa5e702",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 10s 10ms/step - loss: 0.7420 - accuracy: 0.7146\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.7420145273208618, 0.7145504951477051]"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on the whole validation dataset\n",
    "model_2.evaluate(x=valid_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:28.249086288Z",
     "start_time": "2024-05-03T19:54:18.335023567Z"
    }
   },
   "id": "1097ea8d6ad88801",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 9s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.45426244, 0.3353273 , 0.00259361, 0.19985722, 0.0079594 ],\n       [0.36564425, 0.45973203, 0.00599982, 0.16574162, 0.00288224],\n       [0.24894094, 0.17145061, 0.01640184, 0.52776146, 0.03544514],\n       ...,\n       [0.00232965, 0.00649893, 0.05178295, 0.00106503, 0.93832344],\n       [0.00511635, 0.04458313, 0.221475  , 0.00170781, 0.72711766],\n       [0.1771249 , 0.279255  , 0.46440104, 0.0063492 , 0.07286984]],\n      dtype=float32)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with feature extraction model\n",
    "model_2_pred_probs = model_2.predict(valid_dataset)\n",
    "model_2_pred_probs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:37.836579729Z",
     "start_time": "2024-05-03T19:54:28.241903068Z"
    }
   },
   "id": "1fa775a927e73a41",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 3, ..., 4, 4, 2])>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the prediction probabilities found with feature extraction model to labels\n",
    "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
    "model_2_preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:37.851979213Z",
     "start_time": "2024-05-03T19:54:37.834006785Z"
    }
   },
   "id": "f090c8bdc1286651",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 71.45505097312326,\n 'precision': 0.7141585221411326,\n 'recall': 0.7145505097312326,\n 'f1': 0.7118880491867664}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate results from TF Hub pretrained embeddings results on val set\n",
    "model_2_results = evaluate_classification_metrics(y_true=val_labels_encoded,\n",
    "                                                  y_pred=model_2_preds)\n",
    "model_2_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:37.944212838Z",
     "start_time": "2024-05-03T19:54:37.839001681Z"
    }
   },
   "id": "39d4cd31f3eac8f3",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([False, False, False,  True])"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_2_results.values())) > np.array(list(baseline_results.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:37.947280328Z",
     "start_time": "2024-05-03T19:54:37.857309256Z"
    }
   },
   "id": "354a8dd846509eed",
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "source": [
    "**model_2** n'a que le f1 de mieux que le **baseline**, bof bof..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f87a4a58861566"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ° 272: Creating a character-level tokeniser with TensorFlow's TextVectorization layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2ea5f1d6716e3f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 3: Conv1D with character embeddings\n",
    "\n",
    "The paper which we're replicating states they used a combination of token and character-level embeddings.\n",
    "\n",
    "Previously we've token-level embeddings but we'll need to do similar steps for characters if we want to use char-level embeddings. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a71df345bfa9fa2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a character-level tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c07d591f89748e6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .']"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:37.957532911Z",
     "start_time": "2024-05-03T19:54:37.862278324Z"
    }
   },
   "id": "3723b6177a5de30e",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'r e l a p s e d   m u l t i p l e   m y e l o m a   h a s   n o   s t a n d a r d   t r e a t m e n t   ,   a n d   t h e   r o l e   o f   a u t o l o g o u s   s t e m - c e l l   t r a n s p l a n t a t i o n   (   a s c t   )   h a s   n o t   b e e n   f u l l y   d e f i n e d   .'"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "    return \" \".join(list(text))\n",
    "\n",
    "# Text splitting non-character-level sequence into characters\n",
    "split_chars(random_train_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:37.960604288Z",
     "start_time": "2024-05-03T19:54:37.908872194Z"
    }
   },
   "id": "62d29f4ad0b26936",
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .',\n 'a   t o t a l   o f   @   p a t i e n t s   w i t h   p r i m a r y   k n e e   o a   w e r e   r a n d o m i z e d   @ : @   ;   @   r e c e i v e d   @   m g / d a y   o f   p r e d n i s o l o n e   a n d   @   r e c e i v e d   p l a c e b o   f o r   @   w e e k s   .',\n 'o u t c o m e   m e a s u r e s   i n c l u d e d   p a i n   r e d u c t i o n   a n d   i m p r o v e m e n t   i n   f u n c t i o n   s c o r e s   a n d   s y s t e m i c   i n f l a m m a t i o n   m a r k e r s   .',\n 'p a i n   w a s   a s s e s s e d   u s i n g   t h e   v i s u a l   a n a l o g   p a i n   s c a l e   (   @ - @   m m   )   .',\n 's e c o n d a r y   o u t c o m e   m e a s u r e s   i n c l u d e d   t h e   w e s t e r n   o n t a r i o   a n d   m c m a s t e r   u n i v e r s i t i e s   o s t e o a r t h r i t i s   i n d e x   s c o r e s   ,   p a t i e n t   g l o b a l   a s s e s s m e n t   (   p g a   )   o f   t h e   s e v e r i t y   o f   k n e e   o a   ,   a n d   @ - m i n   w a l k   d i s t a n c e   (   @ m w d   )   .']"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split sequence-level data splits into character-level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "train_chars[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:38.376631732Z",
     "start_time": "2024-05-03T19:54:37.909171614Z"
    }
   },
   "id": "faa983bc9b2dc601",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "149.3662574983337"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the average character length?\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_lens)\n",
    "mean_char_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:38.459909330Z",
     "start_time": "2024-05-03T19:54:38.400593005Z"
    }
   },
   "id": "cf34ebd543ac258c",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(array([1.41175e+05, 3.71110e+04, 1.60000e+03, 1.27000e+02, 2.10000e+01,\n        5.00000e+00, 1.00000e+00]),\n array([1.00000000e+00, 1.98857143e+02, 3.96714286e+02, 5.94571429e+02,\n        7.92428571e+02, 9.90285714e+02, 1.18814286e+03, 1.38600000e+03]),\n <BarContainer object of 7 artists>)"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the distribution of our sequences at a character-level\n",
    "plt.hist(char_lens, bins=7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:38.851373235Z",
     "start_time": "2024-05-03T19:54:38.407321515Z"
    }
   },
   "id": "ea7aed4627ded4e7",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "290"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find what character length covers 95% of sequences\n",
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:38.938401560Z",
     "start_time": "2024-05-03T19:54:38.859938391Z"
    }
   },
   "id": "6cd92b12c59b3e19",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all keyboard characters\n",
    "import string\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:38.954745381Z",
     "start_time": "2024-05-03T19:54:38.869299867Z"
    }
   },
   "id": "1ce4e9618a26a227",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create char-level token vectorizer instance\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # add 2 for space and OOV token (OOV = out of vocab, '[UNK]')\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # set standardization to \"None\" if you want to leave punctuation in\n",
    "                                    name=\"char_vectorizer\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:38.964540368Z",
     "start_time": "2024-05-03T19:54:38.877744868Z"
    }
   },
   "id": "887f5185b1fd9205",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Adapt character vectorizer to training character\n",
    "char_vectorizer.adapt(train_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:48.353380199Z",
     "start_time": "2024-05-03T19:54:38.884204654Z"
    }
   },
   "id": "f4ede5b47c8ef603",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different characters in character vocab: 28\n",
      "5 most common characters: ['', '[UNK]', 'e', 't', 'i']\n",
      "5 least common characters: ['k', 'x', 'z', 'q', 'j']\n"
     ]
    }
   ],
   "source": [
    "# Check character vocab stats\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
    "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
    "print(f\"5 least common characters: {char_vocab[-5:]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:48.365739725Z",
     "start_time": "2024-05-03T19:54:48.356398306Z"
    }
   },
   "id": "6f8a331d639e8a84",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charified text:\n",
      " t r a c t - b a s e d   s p a t i a l   s t a t i s t i c s   (   t b s s   )   w a s   u s e d   t o   c o m p a r e   @   s u b j e c t s   w i t h   a d o l e s c e n t   a d h d   w i t h   c o n t r o l   s u b j e c t s   a t   t h e   a g e   o f   @ - @   y e a r s   .\n",
      "\n",
      "Length of random_train_chars: 115\n",
      "\n",
      "Vectorized chars:\n",
      " [[ 3  8  5 11  3 22  5  9  2 10  9 14  5  3  4  5 12  9  3  5  3  4  9  3\n",
      "   4 11  9  3 22  9  9 20  5  9 16  9  2 10  3  7 11  7 15 14  5  8  2  9\n",
      "  16 22 27  2 11  3  9 20  4  3 13  5 10  7 12  2  9 11  2  6  3  5 10 13\n",
      "  10 20  4  3 13 11  7  6  3  8  7 12  9 16 22 27  2 11  3  9  5  3  3 13\n",
      "   2  5 18  2  7 17 19  2  5  8  9  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]]\n",
      "\n",
      "Length of vectorized chars: 290\n"
     ]
    }
   ],
   "source": [
    "# Test out character vectorizer\n",
    "random_train_chars = random.choice(train_chars)\n",
    "print(f\"Charified text:\\n {random_train_chars}\")\n",
    "print(f\"\\nLength of random_train_chars: {len(random_train_chars.split())}\")\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "print(f\"\\nVectorized chars:\\n {vectorized_chars}\")\n",
    "print(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T19:54:48.447482001Z",
     "start_time": "2024-05-03T19:54:48.364354573Z"
    }
   },
   "id": "67da442c59fa9031",
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°273: Creating a character-level embedding layer with tf.keras.layers.Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b610714a8147b5ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a character-level embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bccaff2c4917d75d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create char embedding layer\n",
    "char_embed = layers.Embedding(input_dim=len(char_vocab),  # number of different characters\n",
    "                              output_dim=25, # this is the size of the char embedding in the paper: https://arxiv.org/pdf/1612.05251.pdf (Figure 1)\n",
    "                              mask_zero=True,\n",
    "                              name=\"char_embed\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:02:26.735757066Z",
     "start_time": "2024-05-03T20:02:26.653992665Z"
    }
   },
   "id": "5ab0dfb36665e75f",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charified text:\n",
      " t r a c t - b a s e d   s p a t i a l   s t a t i s t i c s   (   t b s s   )   w a s   u s e d   t o   c o m p a r e   @   s u b j e c t s   w i t h   a d o l e s c e n t   a d h d   w i t h   c o n t r o l   s u b j e c t s   a t   t h e   a g e   o f   @ - @   y e a r s   .\n",
      "\n",
      "Embedded chars (after vectorization and embedding):\n",
      " [[[ 0.02638364 -0.01170463  0.04230449 ...  0.02745718  0.03676036\n",
      "   -0.00448296]\n",
      "  [-0.00508455 -0.04603244 -0.04221971 ... -0.03875373 -0.03312188\n",
      "    0.01833875]\n",
      "  [-0.02677503 -0.04288323 -0.0360782  ... -0.00274001  0.00018071\n",
      "    0.03228701]\n",
      "  ...\n",
      "  [ 0.04828406  0.03469617  0.00749461 ... -0.04159328  0.03237528\n",
      "    0.01999674]\n",
      "  [ 0.04828406  0.03469617  0.00749461 ... -0.04159328  0.03237528\n",
      "    0.01999674]\n",
      "  [ 0.04828406  0.03469617  0.00749461 ... -0.04159328  0.03237528\n",
      "    0.01999674]]]\n",
      "\n",
      "Character embedding shape: (1, 290, 25)\n"
     ]
    }
   ],
   "source": [
    "# Test our character embedding layer\n",
    "print(f\"Charified text:\\n {random_train_chars}\\n\")\n",
    "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
    "print(f\"Embedded chars (after vectorization and embedding):\\n {char_embed_example}\\n\")\n",
    "print(f\"Character embedding shape: {char_embed_example.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:02:27.101494157Z",
     "start_time": "2024-05-03T20:02:27.046571112Z"
    }
   },
   "id": "8be078e0e4c00538",
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°274: Model 3: Building, fitting and evaluating a Conv1D model on character embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a44138bf334fb97"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building a Conv1D model to fit on character embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba4c8b66fdeef613"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Make Conv1D on chars only\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "char_vectors = char_vectorizer(inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "x = layers.Conv1D(filters=64,\n",
    "                  kernel_size=5,\n",
    "                  padding=\"same\",\n",
    "                  activation=\"relu\")(char_embeddings)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(units=num_classes, activation=\"softmax\")(x)\n",
    "model_3 = tf.keras.Model(inputs=inputs,\n",
    "                         outputs=outputs,\n",
    "                         name=\"model_3_conv1d_char_embeddings\")\n",
    "\n",
    "# Compile\n",
    "model_3.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:43:43.575590130Z",
     "start_time": "2024-05-03T20:43:43.506060503Z"
    }
   },
   "id": "a36fe642a12d251a",
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_conv1d_char_embeddings\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " char_vectorizer (TextVecto  (None, 290)               0         \n",
      " rization)                                                       \n",
      "                                                                 \n",
      " char_embed (Embedding)      (None, 290, 25)           1750      \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 290, 64)           8064      \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 64)                0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10139 (39.61 KB)\n",
      "Trainable params: 10139 (39.61 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "model_3.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:43:44.132599052Z",
     "start_time": "2024-05-03T20:43:44.080662869Z"
    }
   },
   "id": "bdddbe1ecdfcc30e",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create char level datasets\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_char_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:43:45.280823864Z",
     "start_time": "2024-05-03T20:43:44.631958352Z"
    }
   },
   "id": "88be5e8e0123659c",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 4s 5ms/step - loss: 1.2404 - accuracy: 0.4941 - val_loss: 1.0401 - val_accuracy: 0.5957\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 3s 5ms/step - loss: 1.0023 - accuracy: 0.6022 - val_loss: 0.9331 - val_accuracy: 0.6360\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 3s 5ms/step - loss: 0.9201 - accuracy: 0.6434 - val_loss: 0.8660 - val_accuracy: 0.6715\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on chars only\n",
    "model_3_history = model_3.fit(x=train_char_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:43:55.753696280Z",
     "start_time": "2024-05-03T20:43:46.160420899Z"
    }
   },
   "id": "844d8d0ebd791836",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.12157943, 0.3693904 , 0.14279795, 0.2514473 , 0.11478491],\n       [0.17503686, 0.48679855, 0.02042137, 0.12335033, 0.19439287],\n       [0.08275625, 0.18238246, 0.45200923, 0.21421407, 0.06863798],\n       ...,\n       [0.01404057, 0.0180261 , 0.09561362, 0.025448  , 0.8468717 ],\n       [0.03549109, 0.14392012, 0.3271341 , 0.03624647, 0.45720825],\n       [0.31830287, 0.45484397, 0.14406215, 0.06381387, 0.0189772 ]],\n      dtype=float32)"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with character model only\n",
    "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
    "model_3_pred_probs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:43:59.631745623Z",
     "start_time": "2024-05-03T20:43:57.113794933Z"
    }
   },
   "id": "a2f9c5822300a118",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 2, ..., 4, 4, 1])>"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to class labels\n",
    "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
    "model_3_preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:44:00.903497898Z",
     "start_time": "2024-05-03T20:44:00.863484036Z"
    }
   },
   "id": "35a15f5d1f342b4c",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 65.87779690189329,\n 'precision': 0.6527563519739392,\n 'recall': 0.6587779690189329,\n 'f1': 0.6495655714433765}"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate results for Conv1D model chars only\n",
    "model_3_results = evaluate_classification_metrics(y_true=val_labels_encoded,\n",
    "                                                  y_pred=model_3_preds)\n",
    "model_3_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:44:01.837103134Z",
     "start_time": "2024-05-03T20:44:01.784235947Z"
    }
   },
   "id": "bf64d80790df2d64",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([False, False, False, False])"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_3_results.values())) > np.array(list(baseline_results.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T20:44:03.312162888Z",
     "start_time": "2024-05-03T20:44:03.259402991Z"
    }
   },
   "id": "781a870e2811d260",
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "source": [
    "**model_3** fait pire que **baseline** ..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b75f0e4edd7da29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video NÂ°275: Discussing how we're going to build Model 4 (character + token embeddings)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34407e94f8b45a24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 4: Combining pretrained token embeddings + characters embeddings (hybrid embedding layer)\n",
    "\n",
    "1. Create a token-level embedding model (similar `model_1`)\n",
    "2. Create a character-level model (similar to `model_3` with a slight modification)\n",
    "3. Combine 1 & 2 with a concatenate (`layers.Concatenate`) \n",
    "4. Build a series of output layers on top of 3 similar to Figure 1 and section 4.2 of [https://arxiv.org/pdf/1612.05251.pdf](https://arxiv.org/pdf/1612.05251.pdf)\n",
    "5. Construct a model which takes token and character-level sequences as input and produces sequence label probabilities as output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3088ef8a1a727c51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a1597f756f724c1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
