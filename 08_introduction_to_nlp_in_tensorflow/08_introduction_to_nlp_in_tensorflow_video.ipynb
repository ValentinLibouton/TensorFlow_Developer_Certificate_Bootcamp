{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to NLP Fundamentals in TensorFlow\n",
    "\n",
    "NLP has the goal of deriving informaton out of natural language (could be seqeuences text or speech).\n",
    "\n",
    "Another common term for NLP problems is sequence to sequence problems (seq2seq).\n",
    "\n",
    "> üìñ **Resource:** See all course materials, resources and extra-curriculum for this notebook on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2adb6a2a9bb4b34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞ 225: Preparing a notebook for our first NLP with TensorFlow project"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "390bc707ee30ca18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check for GPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a01528384bc7a913"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:02.859318002Z",
     "start_time": "2024-04-24T13:29:00.744003027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 15:29:00.998642: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-24 15:29:00.998677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-24 15:29:00.999226: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-24 15:29:01.004075: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 15:29:02.713678: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 15:29:02.744219: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 15:29:02.744537: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 15:29:02.745870: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 15:29:02.746146: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 15:29:02.746395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 15:29:02.808136: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 15:29:02.808312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 15:29:02.808460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-24 15:29:02.808550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5401 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from MachineLearningUtils.system_setup.gpu import activate_gpu\n",
    "activate_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "GPU 0: NVIDIA GeForce RTX 3050 (UUID: GPU-ee6cd8ff-34bf-dc58-2edb-4f5120aaec55)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.079859339Z",
     "start_time": "2024-04-24T13:29:02.855895156Z"
    }
   },
   "id": "a5ad2b6c1c11f0ae",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get helper functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4edd1e8a30d0ca46"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from MachineLearningUtils.data_acquisition.data_downloader import *\n",
    "from MachineLearningUtils.data_visualization.augmentation_effects import *\n",
    "from MachineLearningUtils.data_visualization.evaluation_metrics import *\n",
    "from MachineLearningUtils.data_visualization.image_visualization import *\n",
    "from MachineLearningUtils.data_visualization.model_learning_curves import *\n",
    "from MachineLearningUtils.data_visualization.prediction_visualization import *\n",
    "from MachineLearningUtils.training_utilities.model_callbacks import *\n",
    "from MachineLearningUtils.training_utilities.transfer_learning import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.713703911Z",
     "start_time": "2024-04-24T13:29:03.083550727Z"
    }
   },
   "id": "69b26c93cd437432",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get a text dataset\n",
    "\n",
    "The dataset we're going to be using is Kaggle's introduction to NLP dataset (text samples of Tweets labelled as diaster or not diaster).\n",
    "\n",
    "See the original source here: https://www.kaggle.com/c/nlp-getting-started"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f35c76937f69485"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file nlp_getting_started.zip already exists.\n",
      "Extracting nlp_getting_started.zip as ZIP...\n",
      "nlp_getting_started.zip has been extracted to current directory.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "download_data(url=url, file_path=\"nlp_getting_started.zip\", extract=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.732875481Z",
     "start_time": "2024-04-24T13:29:03.715573498Z"
    }
   },
   "id": "d4fec3deaddbf1f5",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞226: Becoming one with the data and visualising a text dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "855c560ea72bcb02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualizing a text dataset\n",
    "\n",
    "To visualize our text samples, we first have to read them in, one way to do so would be to use Python: https://realpython.com/read-write-files-python/\n",
    "\n",
    "But I prefer to get visual straight away.\n",
    "\n",
    "So another way to do this is to use pandas..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "861f76a64f6f7ad8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.795948053Z",
     "start_time": "2024-04-24T13:29:03.732921786Z"
    }
   },
   "id": "e9417892d138aed0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "        id      keyword               location  \\\n2644  3796  destruction                    NaN   \n2227  3185       deluge                    NaN   \n5448  7769       police                     UK   \n132    191   aftershock                    NaN   \n6845  9810       trauma  Montgomery County, MD   \n\n                                                   text  target  \n2644  So you have a new weapon that can cause un-ima...       1  \n2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n5448  DT @georgegalloway: RT @Galloway4Mayor: ¬â√õ√èThe...       1  \n132   Aftershock back to school kick off was great. ...       0  \n6845  in response to trauma Children of Addicts deve...       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2644</th>\n      <td>3796</td>\n      <td>destruction</td>\n      <td>NaN</td>\n      <td>So you have a new weapon that can cause un-ima...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2227</th>\n      <td>3185</td>\n      <td>deluge</td>\n      <td>NaN</td>\n      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5448</th>\n      <td>7769</td>\n      <td>police</td>\n      <td>UK</td>\n      <td>DT @georgegalloway: RT @Galloway4Mayor: ¬â√õ√èThe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>191</td>\n      <td>aftershock</td>\n      <td>NaN</td>\n      <td>Aftershock back to school kick off was great. ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6845</th>\n      <td>9810</td>\n      <td>trauma</td>\n      <td>Montgomery County, MD</td>\n      <td>in response to trauma Children of Addicts deve...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.835916034Z",
     "start_time": "2024-04-24T13:29:03.761752967Z"
    }
   },
   "id": "18100a531cd6ac08",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the test dataframe look like?\n",
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.837814705Z",
     "start_time": "2024-04-24T13:29:03.804041262Z"
    }
   },
   "id": "de4f346aae86634b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "target\n0    4342\n1    3271\nName: count, dtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df_shuffled.target.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.839330644Z",
     "start_time": "2024-04-24T13:29:03.804310225Z"
    }
   },
   "id": "61b993987dcfacc7",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(7613, 3263)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many total samples?\n",
    "len(train_df), len(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.840785752Z",
     "start_time": "2024-04-24T13:29:03.804500875Z"
    }
   },
   "id": "c3a07e778ad79968",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "@LayLoveTournay @RyroTheUnaware [loud groaning] Let us bang.\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Nice job Calgary transit ! http://t.co/RGOGUyt0LF\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Vehicle Hijacking in  Vosloorus Gauteng on 2015-08-05 at 23:00 White Toyota Conquest  BKB066GP http://t.co/odmP01eyZU\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Perspectives on Terrorism - Understanding Jihadi Proto-States: http://t.co/d5h4jif1y3\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "@JoseBasedGod I'm obliterate you to the shadow realm.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df) - 5) # create random indexes not higher than the total number of samples\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _, text, target= row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.856283887Z",
     "start_time": "2024-04-24T13:29:03.804739572Z"
    }
   },
   "id": "7035319454aaaab6",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞227: Splitting data into training and validation sets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db116c0a3789a71e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split data into training and validation sets\n",
    "\n",
    "We want to be able to see how our model is performing on unseen data whilst it trains.\n",
    "\n",
    "And because the testing dataset doesn't have labels, we'll have to create a validation dataset to evaluate on (the model won't see the validation dataset during training so we can use its samples and labels to evaluate our model's performance)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a2ec748b6ce516d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.858191655Z",
     "start_time": "2024-04-24T13:29:03.804902381Z"
    }
   },
   "id": "21d0b80beabda43a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # use 10% of training data for validation split\n",
    "                                                                            random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.859677720Z",
     "start_time": "2024-04-24T13:29:03.810404128Z"
    }
   },
   "id": "a7f37628542eae88",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(6851, 6851, 762, 762)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:03.871453120Z",
     "start_time": "2024-04-24T13:29:03.815304803Z"
    }
   },
   "id": "74cccbe01bdee96c",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n        'Imagine getting flattened by Kurt Zouma',\n        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n        'destroy the free fandom honestly',\n        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n       dtype=object),\n array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 10 samples\n",
    "train_sentences[:10], train_labels[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:04.003751606Z",
     "start_time": "2024-04-24T13:29:03.820187806Z"
    }
   },
   "id": "b3ddfdb3c4c57617",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞228: Converting text data to numbers using tokenisation and embeddings (overview)\n",
    "Video N¬∞229: Setting up a TensorFlow TextVectorization layer to convert text to numbers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "157a2580c83a5859"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Converting text into numbers\n",
    "\n",
    "When dealing with a text problem, one of the first things you'll have to do before you can build a model is to convert your text to numbers.\n",
    "\n",
    "There are a few ways to do this, namely:\n",
    "* Tokenziation - direct mapping of token (a token could be a word or a character) to number\n",
    "* Embedding - create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26e1e370fdd36669"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text vectorization (tokenization)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8671d415f2b0e06"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n       'Imagine getting flattened by Kurt Zouma',\n       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n       'Somehow find you and I collide http://t.co/Ee8RpOahPk'],\n      dtype=object)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:04.005418042Z",
     "start_time": "2024-04-24T13:29:03.864092425Z"
    }
   },
   "id": "317c903125d50a64",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# Use the default TextVectorization parameters\n",
    "text_vectorizer = TextVectorization(max_tokens=10000, # how many words in the vocabulary (automatically add <OOV>)\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    split=\"whitespace\",\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None, # how long do you want your sequences to be?\n",
    "                                    pad_to_max_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:04.049700342Z",
     "start_time": "2024-04-24T13:29:03.864416539Z"
    }
   },
   "id": "78ef0412598aeab8",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(['@mogacola', '@zamtriossu', 'i', 'screamed', 'after', 'hitting', 'tweet'], 7)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0].split(), len(train_sentences[0].split())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:04.051977067Z",
     "start_time": "2024-04-24T13:29:03.886892157Z"
    }
   },
   "id": "faaf6a9b69bd5773",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the average number of tokens (words) in the training tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:04.054181549Z",
     "start_time": "2024-04-24T13:29:03.899244434Z"
    }
   },
   "id": "28ccda96da25c68f",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Setup text vectorization variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does a model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode='int',\n",
    "                                    output_sequence_length=max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:04.127932279Z",
     "start_time": "2024-04-24T13:29:03.906264426Z"
    }
   },
   "id": "70a82546b3c21d35",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞230: Mapping the TextVectorization layer to text data and turning it into numbers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d12388ae0f3a3e92"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fit the text vectorizer instance to the training data using the adapt() method\n",
    "text_vectorizer.adapt(train_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:04.289456170Z",
     "start_time": "2024-04-24T13:29:03.947889535Z"
    }
   },
   "id": "311370168bb10aeb",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\narray([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]])>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.027711806Z",
     "start_time": "2024-04-24T13:29:04.289747264Z"
    }
   },
   "id": "f2636de95a21fa1",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Free Ebay Sniping RT? http://t.co/B231Ul1O1K Lumbar Extender Back Stretcher Excellent Condition!! ?Please Favorite &amp; Share        \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\narray([[ 268,  462, 4592,   96,    1,    1, 5682,   88,  725, 3874, 3981,\n         170,  853,   35, 1175]])>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "        \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.041604057Z",
     "start_time": "2024-04-24T13:29:05.029254525Z"
    }
   },
   "id": "68bd5bc266492a69",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in vocab: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Least common words in vocab: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]\n",
    "bottom_5_words = words_in_vocab[-5:]\n",
    "print(f\"Most common words in vocab: {top_5_words}\")\n",
    "print(f\"Least common words in vocab: {bottom_5_words}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.095439002Z",
     "start_time": "2024-04-24T13:29:05.039126354Z"
    }
   },
   "id": "d1e651187faa5514",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "10000"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how long is our vocab?\n",
    "len(words_in_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.097574428Z",
     "start_time": "2024-04-24T13:29:05.079938352Z"
    }
   },
   "id": "c3ed2e4f338a2fb4",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞231: Creating an Embedding layer to turn tokenised text into embedding vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fae42820e8cdf2c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating an Embedding using an Embedding Layer\n",
    "\n",
    "To make our embedding, we're going to use TensorFlow's embedding layer:\n",
    "[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
    "The parameters we care most about for our embedding layer:\n",
    "- `input_dim` = the size of our vocabulary\n",
    "- `output_dim` = the size of the output embedding vector, for example, a value of 100 would mean each token gets represented by a vector 100 long\n",
    "- `input_length` = length of the sequences being passed to the embedding layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42657b0b2a7e2783"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.src.layers.core.embedding.Embedding at 0x7fc3f4555210>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set the input shape\n",
    "                             output_dim=128, # set the size of the embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, initialize embedding vectors randomly\n",
    "                             input_length=max_length) # how long is each input\n",
    "embedding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.111869367Z",
     "start_time": "2024-04-24T13:29:05.080225118Z"
    }
   },
   "id": "51adde2a2596e334",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Now playing: Boat Club - Memories on London Burning Web Radio - http://t.co/umtNNImTbM\n",
      "\n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\narray([[[ 0.01119319,  0.02730015, -0.03706022, ..., -0.0312544 ,\n         -0.02477062, -0.02139863],\n        [ 0.03866917,  0.0061719 , -0.02475965, ..., -0.03538996,\n          0.02468387, -0.04343798],\n        [-0.02376082, -0.02621559, -0.00488853, ..., -0.02071044,\n         -0.04759247, -0.01124054],\n        ...,\n        [-0.04052677,  0.0377803 ,  0.00385106, ..., -0.02789776,\n          0.04598102,  0.01467085],\n        [-0.04052677,  0.0377803 ,  0.00385106, ..., -0.02789776,\n          0.04598102,  0.01467085],\n        [-0.04052677,  0.0377803 ,  0.00385106, ..., -0.02789776,\n          0.04598102,  0.01467085]]], dtype=float32)>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f'Original text:\\n {random_sentence}')\n",
    "print(f'\\n\\nEmbedded version:')\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.179840621Z",
     "start_time": "2024-04-24T13:29:05.080463345Z"
    }
   },
   "id": "cfbc705b8df7725e",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n array([ 1.11931935e-02,  2.73001529e-02, -3.70602235e-02, -1.01143233e-02,\n         1.72715224e-02, -3.69562618e-02, -1.67324916e-02, -2.17650067e-02,\n         3.67444418e-02,  3.03609483e-02, -3.87267694e-02,  6.05789572e-03,\n        -4.73464988e-02,  1.69933327e-02, -3.98011096e-02, -2.36828923e-02,\n         4.28791530e-02, -4.25588340e-04,  1.70431398e-02, -2.34944355e-02,\n        -3.32148671e-02, -3.29403505e-02, -3.07498462e-02, -2.69837305e-03,\n         9.88473743e-03,  3.17744277e-02,  2.36407407e-02, -3.71210575e-02,\n        -2.57665757e-02, -3.48693021e-02,  3.33171003e-02,  3.56338657e-02,\n        -9.21286270e-03, -2.99336314e-02,  2.94231884e-02, -2.01848503e-02,\n        -3.52154151e-02,  1.04357377e-02,  3.55880335e-03, -2.54676472e-02,\n         4.42687906e-02, -9.45084170e-03, -1.50659196e-02,  4.54017781e-02,\n        -1.02604516e-02, -4.65494059e-02, -3.07433605e-02,  8.89562070e-04,\n         8.41879845e-03, -5.29267639e-03,  4.55878116e-02,  4.57512029e-02,\n        -1.93684828e-02,  4.70504873e-02,  1.16199367e-02,  8.97580385e-03,\n        -4.19437885e-02,  1.30312108e-02,  3.35438177e-03, -3.49349491e-02,\n        -1.76019594e-03, -6.22164086e-03, -3.18055041e-02,  3.50663327e-02,\n         3.78465168e-02,  1.66275017e-02,  2.96989568e-02,  2.08115093e-02,\n         4.00933288e-02,  3.96615528e-02,  2.80770995e-02,  2.77781971e-02,\n         1.93534382e-02,  1.22590438e-02,  1.29934400e-03,  2.45837308e-02,\n         4.08318676e-02, -1.16808787e-02,  6.46498054e-03,  5.62239811e-03,\n        -2.74150129e-02, -1.66050419e-02,  9.87782329e-03,  2.95996778e-02,\n         1.60966404e-02, -2.59806514e-02,  3.39896418e-02, -3.40738297e-02,\n         3.30020115e-03,  1.54744126e-02,  2.40973867e-02,  3.01381089e-02,\n         1.92465819e-02, -3.70144844e-05,  4.99521382e-02, -2.54479647e-02,\n        -2.70410534e-02, -3.78228649e-02,  2.81876586e-02, -1.91700589e-02,\n        -1.84626356e-02, -3.55818272e-02,  3.60800661e-02, -3.94909456e-03,\n         4.07136194e-02,  4.58224677e-02, -4.45136912e-02,  2.92926170e-02,\n        -4.61083651e-02, -3.38848606e-02,  3.62620391e-02, -4.19913754e-02,\n         1.11703984e-02, -7.96788931e-03, -2.78750304e-02,  3.89139093e-02,\n         2.20992677e-02,  2.68621482e-02, -2.02672482e-02,  4.98076566e-02,\n        -9.90469381e-03,  1.49676092e-02,  1.34825818e-02,  4.64923494e-02,\n         4.35186289e-02, -3.12543996e-02, -2.47706175e-02, -2.13986281e-02],\n       dtype=float32)>,\n TensorShape([128]),\n 'Now playing: Boat Club - Memories on London Burning Web Radio - http://t.co/umtNNImTbM')"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.184187160Z",
     "start_time": "2024-04-24T13:29:05.112260364Z"
    }
   },
   "id": "7f7aea54142f9074",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞232: Discussing the various modelling experiments we're going to run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a97c61909d8b52c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modelling a text dataset (setting up our modelling experiments)\n",
    "\n",
    "Now we've a got way to turn our text sequences into numbers, it's time to start building a series of modelling experiments.\n",
    "\n",
    "We'll start with a baseline and move on from there.\n",
    "\n",
    "* Model 0: Naive Bayes (baseline) - got this from here: [https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "* Model 1: Feed-forward neural network (dense model)\n",
    "* Model 2: LSTM model (RNN)\n",
    "* Model 3: GRU model (RNN)\n",
    "* Model 4: Bidirectional LSTM model (RNN)\n",
    "* Model 5: 1D Convolutional Neural Network (CNN)\n",
    "* Model 6: TensorFlow Hub Pretrained Word Embedding (feature extractor) (using transfer learning for NLP)\n",
    "* Model 7: Same as model 6 but using 10% of data\n",
    "\n",
    "How are we going to approach all of these?\n",
    "Use the standard steps in modelling with tensorflow:\n",
    "* Create a model\n",
    "* Build a model\n",
    "* Fit a model\n",
    "* Evaluate our model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5685fc4726befe9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞233: Model 0:Building a basline model to try and improve upon"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46ceb5c15d31e587"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 0: Getting a baseline\n",
    "\n",
    "As with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiments to build upon.\n",
    "\n",
    "To create our baseline, we'll use Sklearn's Multinomial Naive Bayes using the TF-IDF formula to convert our words to numbers.\n",
    "\n",
    "> üîë **Note:** It's common practice to use non-DL algorithms as a baseline because of their speed and then later using DL to see if you can improve upon them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "351db476be91e232"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 232 ms, sys: 59 ¬µs, total: 232 ms\n",
      "Wall time: 230 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])",
      "text/html": "<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"‚ñ∏\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"‚ñæ\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div></div></div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    (\"clf\", MultinomialNB()) # model the text converted to numbers\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.365810393Z",
     "start_time": "2024-04-24T13:29:05.125964390Z"
    }
   },
   "id": "ece154ec6304d143",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.382673461Z",
     "start_time": "2024-04-24T13:29:05.373016174Z"
    }
   },
   "id": "91fd67891a4f78aa",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.435306522Z",
     "start_time": "2024-04-24T13:29:05.387929668Z"
    }
   },
   "id": "6424a4c08002f460",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 1, ..., 1, 1, 0])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.470772398Z",
     "start_time": "2024-04-24T13:29:05.395438094Z"
    }
   },
   "id": "1bf16bacada8c7a",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞234: Creating a function to track and evaluate our model's results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f880313bcceb06d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating an evaluation function for our model experiments\n",
    "\n",
    "We could evaluate all of our model's predictions with different metrics every time, however, this will be cumbersome and could easily be fixed with a function\n",
    "\n",
    "Let's create one to compare our model's predictions with the truth labels using the following metrics:\n",
    " \n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "\n",
    "For a deep overview of many different evaluation methods, see the Sklearn documentation: [https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb8c0e70525c315d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 79.26509186351706,\n 'precision': 0.8111390004213173,\n 'recall': 0.7926509186351706,\n 'f1': 0.7862189758049549}"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results with personal function\n",
    "baseline_results = evaluate_classification_metrics(y_true=val_labels, y_pred=baseline_preds)\n",
    "baseline_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.472377411Z",
     "start_time": "2024-04-24T13:29:05.435892446Z"
    }
   },
   "id": "db41551daf616ee1",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞235: Model1: Building, fitting and evaluating our first deep model on text data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c8e31a1da657d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 1: A simple dense model (feed-forward neural network)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3c026a9b82c23c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create tensorboard callback (need to create a new one for each model)\n",
    "# import personal 'create_tensorboard_callback' function\n",
    "\n",
    "# Create directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.482719273Z",
     "start_time": "2024-04-24T13:29:05.436117549Z"
    }
   },
   "id": "caf0fc8f8529b5b3",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n       'Imagine getting flattened by Kurt Zouma',\n       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n       'Somehow find you and I collide http://t.co/Ee8RpOahPk'],\n      dtype=object)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.484820415Z",
     "start_time": "2024-04-24T13:29:05.436275899Z"
    }
   },
   "id": "dc538b4c25a084d1",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Build model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string) # inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers \n",
    "x = embedding(x) # create an embedding of the numberized inputs\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the feature vector for each token to one vector\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation function\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.486707405Z",
     "start_time": "2024-04-24T13:29:05.436431444Z"
    }
   },
   "id": "efe97384acbcfdf7",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:05.573815556Z",
     "start_time": "2024-04-24T13:29:05.464155015Z"
    }
   },
   "id": "5380ee4a34e8ca0c",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20240424-152905\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 15:29:06.860591: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fc3a48245f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-24 15:29:06.860620: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050, Compute Capability 8.6\n",
      "2024-04-24 15:29:06.865820: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-24 15:29:06.879156: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713965346.950520   20895 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 8s 30ms/step - loss: 0.6120 - accuracy: 0.6853 - val_loss: 0.5349 - val_accuracy: 0.7572\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.4420 - accuracy: 0.8180 - val_loss: 0.4713 - val_accuracy: 0.7874\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.3471 - accuracy: 0.8608 - val_loss: 0.4601 - val_accuracy: 0.7874\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2837 - accuracy: 0.8913 - val_loss: 0.4611 - val_accuracy: 0.7940\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2373 - accuracy: 0.9130 - val_loss: 0.4766 - val_accuracy: 0.7795\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "history_1 = model_1.fit(train_sentences,\n",
    "                        train_labels,\n",
    "                        epochs=5,\n",
    "                        validation_data=(val_sentences, val_labels),\n",
    "                        callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                               experiment_name=\"model_1_dense\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:19.637254493Z",
     "start_time": "2024-04-24T13:29:05.508461711Z"
    }
   },
   "id": "a34623962e038497",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.4766 - accuracy: 0.7795\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.47655874490737915, 0.7795275449752808]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model 1\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:19.757558518Z",
     "start_time": "2024-04-24T13:29:19.640596741Z"
    }
   },
   "id": "b01595cecb296007",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.5104694 ],\n       [0.764053  ],\n       [0.99748   ],\n       [0.14878684],\n       [0.13542274],\n       [0.9406969 ],\n       [0.91244453],\n       [0.99333864],\n       [0.97388124],\n       [0.31224036]], dtype=float32)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first 10 predictions\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:19.937602485Z",
     "start_time": "2024-04-24T13:29:19.759948471Z"
    }
   },
   "id": "367413fb65f0e20d",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n       0., 0., 1.], dtype=float32)>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model prediction\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:19.989750037Z",
     "start_time": "2024-04-24T13:29:19.932431574Z"
    }
   },
   "id": "a71c327710295aee",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'accuracy': 77.95275590551181,\n 'precision': 0.7808713829501961,\n 'recall': 0.7795275590551181,\n 'f1': 0.7777699173240801}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caluclate our model_1 results\n",
    "model_1_results = evaluate_classification_metrics(y_true=val_labels,\n",
    "                                                  y_pred=model_1_preds)\n",
    "model_1_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:29:20.034576245Z",
     "start_time": "2024-04-24T13:29:19.990227232Z"
    }
   },
   "id": "a782f9033b583334",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([False, False, False, False])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:34:14.433376082Z",
     "start_time": "2024-04-24T13:34:14.422207741Z"
    }
   },
   "id": "9b0eb497929e500a",
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous n'avons donc pas de meilleurs r√©sultats avec model_1 compar√© √† model_0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c66324dd91cf3fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞236: Visualising our model's learned word embeddings with TensorFlow's projector tool"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89d57375dca9de72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizing learned embeddings\n",
    "\n",
    "To further understand word embeddings, let's visualize them, to do so, we'll get the weights matrix (embedding matrix) from our embedding layer and visualize it using the Embedding project tool, see the TensorFlow guide for more: https://www.tensorflow.org/tutorials/text/word_embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4063743e80fe09f9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary from the text vectorization layer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T17:26:16.906644205Z",
     "start_time": "2024-04-24T17:26:16.860704729Z"
    }
   },
   "id": "9ff40e4b67bcc70e",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "10000"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_in_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T15:24:11.629339065Z",
     "start_time": "2024-04-24T15:24:11.573976944Z"
    }
   },
   "id": "12fddbef57b5a99f",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "10000"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab_length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T17:26:58.683598507Z",
     "start_time": "2024-04-24T17:26:58.639728372Z"
    }
   },
   "id": "a673c94114a8560c",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 1 summary\n",
    "model_1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T17:27:24.360329115Z",
     "start_time": "2024-04-24T17:27:24.306712255Z"
    }
   },
   "id": "6ce2d1c8b6e6fbe8",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Get the weight matrix of embedding layer\n",
    "# (these are the numerical representations of each token in our training data, which have been learned for 5 epochs)\n",
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()[0]\n",
    "print(embed_weights.shape) # same size as vocab size and embedding_dim (output_dim of our embedding layer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T17:34:49.161334292Z",
     "start_time": "2024-04-24T17:34:49.119257060Z"
    }
   },
   "id": "b35772353fac8bf2",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we've got the embedding matrix our model has learned to represent our tokens, let's see how we can visualize it.\n",
    "\n",
    "To do so, TensorFlow has a handy tool called projector: [https://projector.tensorflow.org/](https://projector.tensorflow.org/)\n",
    "\n",
    "And TensorFlow also has an incredible guide on word embeddings themselves: [https://www.tensorflow.org/text/guide/word_embeddings](https://www.tensorflow.org/text/guide/word_embeddings)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddba71ea33ebbc85"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create embedding files (we got this from TensorFlow's word embedding documentation)\n",
    "import io\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = embed_weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T19:34:47.860257089Z",
     "start_time": "2024-04-24T19:34:47.129139648Z"
    }
   },
   "id": "617b08c60c93aca8",
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are running this tutorial in Colaboratory, you can use the following snippet to download these files to your local machine (or use the file browser, View -> Table of contents -> File browser).\n",
    "```python\n",
    "# Download files from Colab to upload projector\n",
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fb9b51ae251709c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our visual word embeddings might not look like much to us, but they help our model understand the relationships between words.\n",
    "\n",
    "For more on a popular type of word embedding and more visual explanations check out the illustrated word2vec: [https://jalammar.github.io/illustrated-word2vec/](https://jalammar.github.io/illustrated-word2vec/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77603eaf4b309be9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞237: High-level overview of Recurrent Networks (RNNs) + where to learn more"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e92e76eacd017a41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recurrent Neural Networks (RNN's)\n",
    "\n",
    "RNN's are useful for sequence data.\n",
    "\n",
    "The premise of a recurrent neural network is to use the representation of a previous input to aid the representation of a later input.\n",
    "\n",
    "**Resources:** If you want an overview of the internals of a recurrent neural network, see the following:\n",
    "- MIT's sequence modelling lecture [https://www.youtube.com/watch?v=qjrad0V0uJE](https://www.youtube.com/watch?v=qjrad0V0uJE)\n",
    "- Chris Olah's intro to LSTMs: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Andrej Karpathy's the unreasonable effectiveness of recurrent neural networks: [https://karpathy.github.io/2015/05/21/rnn-effectiveness/](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61ceae431627f8e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Video N¬∞238: Model 2: Building, fitting and evaluating our first TensorFlow RNN model (LSTM)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce04b6bb81edb89b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a0677b0d7243c821"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
