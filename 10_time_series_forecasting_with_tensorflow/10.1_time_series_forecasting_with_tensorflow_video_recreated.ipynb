{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e86e16",
   "metadata": {},
   "source": [
    "# Time series forecasting fundamentals with TensorFlow + Milestone Project 3: BitPredict 💰📈\n",
    "\n",
    "Let's write some TensorFlow code to predict the price of Bitcoin based on the historical price of Bitcoin.\n",
    "\n",
    "⚠️ **Note:** This is not financial advice.\n",
    "\n",
    "Links:\n",
    "* All resources: [https://github.com/mrdbourke/tensorflow-deep-learning](https://github.com/mrdbourke/tensorflow-deep-learning)\n",
    "* Book version of actual code/text: [https://dev.mrdbourke.com/tensorflow-deep-learning/10_time_series_forecasting_in_tensorflow/](https://dev.mrdbourke.com/tensorflow-deep-learning/10_time_series_forecasting_in_tensorflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89be06",
   "metadata": {},
   "source": [
    "Video N°299: Downloading and inspecting our Bitcoin historical dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f854fda7",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "We're going to be using the historical price data of Bitcoin to try and predict the future price of Bitcoin, we downloaded the data from here: https://www.coindesk.com/price/bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2298fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MachineLearningUtils.data_acquisition.data_downloader import *\n",
    "url = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\"\n",
    "download_data(url=url, file_path=\"BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5616f30",
   "metadata": {},
   "source": [
    "## Importing time series with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f011bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import with pandas\n",
    "import pandas as pd\n",
    "# Let's read in our Bitcoin data and parse the dates\n",
    "df = pd.read_csv('BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv',\n",
    "                 parse_dates=[\"Date\"],\n",
    "                 index_col=[\"Date\"]) # prase the date column and tell pandas column 1 is a datetime\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0de181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9bb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2513f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples do we have?\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f52f5d6",
   "metadata": {},
   "source": [
    "We've collected the historical price of Bitcoin for the past ~8 years but there's 2787 samples.\n",
    "\n",
    "Typically deep learning models usually like lots and lots and lots of samples (where lots and lots and lots can thousands to tens of thousands to millions).\n",
    "\n",
    "A smaller number of samples is something you'll often run into with time series data problems.\n",
    "\n",
    "> 🔑 **Note:** The **seasonality** of a time series dataset is referred as the number of samples per year. So for our Bitcoin data, it has a seaonality of daily or value of 365 because we collect one sample per day meaning we'll get 365 samples per year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b5213",
   "metadata": {},
   "source": [
    "Video N°300: Different kinds of time series patterns & different amounts of feature variables\n",
    "Video N°301: Visualizing our Bitcoin historical data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only want closing price for each day\n",
    "bitcoin_prices = pd.DataFrame(df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\":\"Price\"})\n",
    "bitcoin_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06038073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bitcoin_prices.plot(figsize=(10,7))\n",
    "plt.ylabel(\"BTC Price\")\n",
    "plt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16)\n",
    "plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3181a",
   "metadata": {},
   "source": [
    "Video N°302: Reading in our Bitcoin data with Python's CSV module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and formatting historical Bitcoin data with Python\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "timesteps = []\n",
    "btc_price = []\n",
    "with open(file=\"BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", mode='r') as f:\n",
    "    csv_reader = csv.reader(f, delimiter=\",\")\n",
    "    next(csv_reader) # skip first line (this gets rid of the column titles)\n",
    "    for line in csv_reader:\n",
    "        timesteps.append(datetime.strptime(line[1], \"%Y-%m-%d\")) # get the dates as dates (not strings)\n",
    "        btc_price.append(float(line[2])) # get the closing price as float\n",
    "# View first 10 of each\n",
    "timesteps[:10], btc_price[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a966ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot from CSV\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(timesteps, btc_price)\n",
    "plt.ylabel(\"BTC Price\")\n",
    "plt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16)\n",
    "plt.xlabel(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e11a3",
   "metadata": {},
   "source": [
    "Video N°303: Creating train and test splits for time series (the wrong way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152688f3",
   "metadata": {},
   "source": [
    "## Format data part 1: Create train and test sets of our time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02c03b",
   "metadata": {},
   "source": [
    "### Creating train and test sets with time series data (the wrong way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_prices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bitcoin date array\n",
    "timesteps = bitcoin_prices.index.to_numpy()\n",
    "prices = bitcoin_prices[\"Price\"].to_numpy()\n",
    "\n",
    "timesteps[:10], prices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69afb374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong way to make train/test sets for time series data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(timesteps,\n",
    "                                                    prices,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot wrong train and test splits\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(X_train, y_train, s=5, label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, s=5, label=\"Test data\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"BTC Price\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827aea8",
   "metadata": {},
   "source": [
    "Video N°304: Creating train and test splits for time series (the right way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05714359",
   "metadata": {},
   "source": [
    "### Create train & test sets for time series (the right way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test splits the right way for time series data\n",
    "split_size = int(0.8 * len(prices)) # 80% train, 20% test - you can change these values as needed, e.g. 90/10, 95/5\n",
    "\n",
    "# Create train data splits (everything before the split)\n",
    "X_train, y_train = timesteps[:split_size], prices[:split_size]\n",
    "\n",
    "# Create test data splits (everything beyond the split)\n",
    "X_test, y_test = timesteps[split_size:], prices[split_size:]\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e06905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correctly made splits\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(X_train, y_train, s=5, label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, s=5, label=\"Test data\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"BTC Price\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0a71a",
   "metadata": {},
   "source": [
    "Video N°305: Creating a plotting function to visualize our time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d3140",
   "metadata": {},
   "source": [
    "## Create a plotting function\n",
    "\n",
    "Typing plotting code is tedious, let's functionize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(timesteps, values, format=\".\", start=0, end=None, label=None):\n",
    "    \"\"\"\n",
    "    Plots timesteps (a series of points in time) against values (a series of values across timesteps).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    timesteps : array of timestep values\n",
    "    values : array of values across time\n",
    "    format : style of plot, default \".\"\n",
    "    start : where to start the plot (setting a value will index from start of timesteps & values)\n",
    "    end : where to end the plot (similar to start but for the end)\n",
    "    label : label to show on plot about values, default None \n",
    "    \"\"\"\n",
    "    # Plot the series\n",
    "    plt.plot(timesteps[start:end], values[start:end], format, label=label)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"BTC Price\")\n",
    "    if label:\n",
    "        plt.legend(fontsize=14) # make label bigger\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3180293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out plotting function\n",
    "plt.figure(figsize=(10,7))\n",
    "plot_time_series(timesteps=X_train, values=y_train,label=\"Train data\")\n",
    "plot_time_series(timesteps=X_test, values=y_test,label=\"Test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a12f7",
   "metadata": {},
   "source": [
    "Video N°306: Discussing the various modelling experiments were going to be running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0158e65",
   "metadata": {},
   "source": [
    "## Modelling Experiments\n",
    "\n",
    "We've got some Bitcoin historical data, to model it, let's run a series of modelling experiments and see which model performs best.\n",
    "\n",
    "Terms to be familiar with:\n",
    "* **Horizon** = number of timesteps into the future we're going to predict\n",
    "* **Window size** = number of timesteps we're going to use to predict **horizon**\n",
    "\n",
    "Modelling experiments we're running:\n",
    "* 0\tNaïve model (baseline)\t\n",
    "* 1\tDense model, horizon = 1, window = 7\n",
    "* 2\tSame as 1, \thorizon = 1, window = 30\n",
    "* 3\tSame as 1, \thorizon = 7, window =\t30\n",
    "* 4\tConv1D\n",
    "* 5\tLSTM\n",
    "* 6\tSame as 1 (but with multivariate data)\n",
    "* 7\tN-BEATs Algorithm\n",
    "* 8\tEnsemble (multiple models optimized on different loss functions)\n",
    "* 9\tFuture prediction model (model to predict future values)\n",
    "* 10\tSame as 1 (but with turkey 🦃 data introduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74432da9",
   "metadata": {},
   "source": [
    "Video N°307: Model 0:Making and visualizing a naive forecast model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600aec7b",
   "metadata": {},
   "source": [
    "## Model 0: Naive forecast (baseline)\n",
    "\n",
    "The formula looks like this:\n",
    "\n",
    "$$\\hat{y}_{t} = y_{t-1}$$\n",
    "\n",
    "In English:\n",
    "\n",
    "> The prediction at timestep t (y-hat) is equal to the value at timestep t-1 (previous timestep) - this is for a horizon of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b239a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a naive forecast\n",
    "naive_forecast = y_test[:-1]\n",
    "\n",
    "naive_forecast[:10], naive_forecast[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20deab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot naive forecast\n",
    "plt.figure(figsize=(20,14))\n",
    "# plot_time_series(timesteps=X_train, values=y_train,label=\"Train data\")\n",
    "plot_time_series(timesteps=X_test, values=y_test, start=350, format=\"-\", label=\"Test data\")\n",
    "plot_time_series(timesteps=X_test[1:], values=naive_forecast, start=350, format=\"-\", label=\"Naive forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c5a6d",
   "metadata": {},
   "source": [
    "> 🛠 **Exercise:** Spend 10 minutes reading the [simple forecasts chapter of Forecasting: Principles in Practice](https://otexts.com/fpp3/simple-methods.html). And have a search to find out why the naive forecast is hard to beat, does the term augoregressive come up? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63413c",
   "metadata": {},
   "source": [
    "Video N°308: Discussing some of the most common time series evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6480eae",
   "metadata": {},
   "source": [
    "## Evaluating a time series model\n",
    "\n",
    "Let's look into some evaluation metrics for time series forecasting. \n",
    "\n",
    "What are we doing?\n",
    "\n",
    "We're predicting a number, so that means we have a form of a regression problem.\n",
    "\n",
    "Because we're working on a regression problem, we'll need some regression-like metrics.\n",
    "\n",
    "A few common regression metrics (which can also be used for time series forecasting):\n",
    "* MAE - mean absolute error\n",
    "* MSE - mean squared error\n",
    "* RMSE - root mean square error\n",
    "* MAPE/sMAPE - (symmetric) mean absolute percentage error\n",
    "* MASE - mean absolute scaled error\n",
    "\n",
    "For all of the above metrics, **lower is better**, for example, an MAE of 0 that is better than an MAE of 100.\n",
    "\n",
    "The main thing we're evaluating here is: **how do our model's forecasts (y_pred) compare against the actual values (y_true or ground truth values)?**\n",
    "\n",
    "> 📖 **Resource:** For a deeper dive on the various kinds of time series forecasting methods see [Forecasting: Principles and Practice chapter 5.8](https://otexts.com/fpp3/accuracy.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de38021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASE implementation\n",
    "def mean_absolute_scaled_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Implement MASE (assuming no seasonality of data).\n",
    "    \"\"\"\n",
    "    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "    \n",
    "    # Find MAE of naive forecast (no seasonality)\n",
    "    mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # our seasonality is 1 day (hence the shift of 1)\n",
    "    return mae / mae_naive_no_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb763fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MASE (this value should = 1 or be very close to 1 with the naive forecast)\n",
    "mean_absolute_scaled_error(y_true=y_test[1:], y_pred=naive_forecast).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b4e05",
   "metadata": {},
   "source": [
    "Video N°310: Creating a function to evaluate our model's forecasts with various metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to take in model predictions and truth values and return evaluation metrics\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    # Make sure float32 datatype (for metric calculations)\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    \n",
    "    # Calculate various evaluation metrics \n",
    "    mae = tf.keras.metrics.mean_absolute_error(y_true=y_true, y_pred=y_pred)\n",
    "    mse = tf.keras.metrics.mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true=y_true, y_pred=y_pred)\n",
    "    mase = mean_absolute_scaled_error(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    return {\"mae\": mae.numpy(),\n",
    "            \"mse\": mse.numpy(),\n",
    "            \"rmse\": rmse.numpy(),\n",
    "            \"mape\": mape.numpy(),\n",
    "            \"mase\": mase.numpy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337ad80",
   "metadata": {},
   "source": [
    "That's one good looking evaluation function! \n",
    "\n",
    "Let's now see if it works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_results = evaluate_preds(y_true=y_test[1:], y_pred=naive_forecast)\n",
    "naive_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df263b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_forecast[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_min(y_test), tf.reduce_max(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d0d23",
   "metadata": {},
   "source": [
    "Video N°311: Discussing other non-TensorFlow kinds of time series forecasting models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243bf8ad",
   "metadata": {},
   "source": [
    "## Other models you can use for baselines and for actual forecasts\n",
    "\n",
    "In this notebook, we're focused on TensorFlow and deep learning models.\n",
    "\n",
    "However there are plenty of other styles of time series forecasting models you may want to experiment with: [https://dev.mrdbourke.com/tensorflow-deep-learning/10_time_series_forecasting_in_tensorflow/#other-kinds-of-time-series-forecasting-models-which-can-be-used-for-baselines-and-actual-forecasts](https://dev.mrdbourke.com/tensorflow-deep-learning/10_time_series_forecasting_in_tensorflow/#other-kinds-of-time-series-forecasting-models-which-can-be-used-for-baselines-and-actual-forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c78e6",
   "metadata": {},
   "source": [
    "Video N°312: Formatting data Part 2: Creating a function to label our windowed time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02efa6a",
   "metadata": {},
   "source": [
    "## Format Data Part 2: Windowing our dataset\n",
    "\n",
    "Why do we window?\n",
    "\n",
    "We window our time series dataset to turn our data into a supervised learning problem.\n",
    "\n",
    "```\n",
    "Windowing for one week\n",
    "[0, 1, 2, 3, 4, 5, 6] -> [7]\n",
    "[1, 2, 3, 4, 5, 6, 7] -> [8]\n",
    "[2, 3, 4, 5, 6, 7, 8] -> [9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we want to do with our Bitcoin data\n",
    "print(f\"We want to use: {btc_price[:7]} to predict this: {btc_price[7]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06688845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's setup global variables for window and horizon size\n",
    "HORIZON = 1 # predict next 1 day\n",
    "WINDOW_SIZE = 7 # use the past week of Bitcoin data to make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87cb7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to label windowed data\n",
    "def get_labelled_windows(x, horizon=HORIZON):\n",
    "    \"\"\"\n",
    "    Creates labels for windowed dataset.\n",
    "    \n",
    "    E.g. if horizon=1\n",
    "    Input: [0, 1, 2, 3, 4, 5, 6, 7] -> Output: ([0, 1, 2, 3, 4, 5, 6], [7])\n",
    "    \"\"\"\n",
    "    return x[:, :-horizon], x[:, -horizon:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59425e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the window labelling function\n",
    "test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8), axis=0))\n",
    "print(f\"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028120f",
   "metadata": {},
   "source": [
    "Video N°313: Discussing the use of windows and horizons in time series data\n",
    "Video N°314: Writing a preprocessing function to turn time series data into windows & labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb356ce",
   "metadata": {},
   "source": [
    "We've got a way to label our windowed data.\n",
    "\n",
    "However, this only works on a small scale.\n",
    "\n",
    "We need a way to do the above across our entire time series.\n",
    "\n",
    "We could do this with Python for loops, however, for large time series, that'd be quite slow.\n",
    "\n",
    "To speed things up, we'll leverage NumPy's array indexing - https://numpy.org/doc/stable/reference/arrays.indexing.html.\n",
    "\n",
    "Our function will:\n",
    "1. Create a window step of specific window size (e.g. [0, 1, 2, 3, 4, 5, 6])\n",
    "2. Use NumPy indexing to create a 2D array of multiple window steps, for example: \n",
    "```\n",
    "[[0, 1, 2, 3, 4, 5, 6],\n",
    "[1, 2, 3, 4, 5, 6, 7],\n",
    "[2, 3, 4, 5, 6, 7, 8]]\n",
    "```\n",
    "3. Uses the 2D array of multiple window steps (from 2.) to index on a target series (e.g. the historical price of Bitcoin)\n",
    "4. Uses our `get_labelled_windows()` function we created above to turn the window steps into windows with a specified horizon \n",
    "\n",
    "> 📖 **Resource:** The function we're about to create has been adapted from the following article: https://towardsdatascience.com/fast-and-robust-sliding-window-vectorization-with-numpy-3ad950ed62f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ab748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create function to view NumPy arrays as windows\n",
    "def make_windows(x, window_size=WINDOW_SIZE, horizon=HORIZON):\n",
    "    \"\"\"\n",
    "    Turns a 1D array into a 2D array of sequential labelled windows of window_size with horizon size labels.\n",
    "    \"\"\"\n",
    "    # 1. Create a window of specific window_size (add the horzion on the end for labelling later)\n",
    "    window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
    "    \n",
    "    # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
    "    window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
    "    # print(f\"Window indexes:\\n {window_indexes, window_indexes.shape}\")\n",
    "    \n",
    "    # 3. Index on the target array (a time series) with 2D array of multiple window steps\n",
    "    windowed_array = x[window_indexes]\n",
    "    # print(windowed_array)\n",
    "    \n",
    "    # 4. Get the labelled windows\n",
    "    windows, labels = get_labelled_windows(x=windowed_array,horizon=horizon)\n",
    "    return windows, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eddc790",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_windows, full_labels = make_windows(x=prices, window_size=WINDOW_SIZE, horizon=HORIZON)\n",
    "len(full_windows), len(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0655be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 3 windows/labels\n",
    "for i in range(3):\n",
    "    print(f\"Window: {full_windows[i]} -> Label: {full_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31472615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last 3 windows/labels\n",
    "for i in range(3):\n",
    "  print(f\"Window: {full_windows[i-3]} -> Label: {full_labels[i-3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a0aaa",
   "metadata": {},
   "source": [
    "> 🔑 **Note:** There's a function which does similar to the above in tf.keras.preprocessing, an extension could be to try and replicate what we've done using the premade function: [https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb3e81",
   "metadata": {},
   "source": [
    "Video N°315: Turning our windowed time series data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25532354",
   "metadata": {},
   "source": [
    "## Turning windows into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_windows[:5], full_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the train/test splits\n",
    "def make_train_test_splits(windows, labels, test_split=0.2):\n",
    "    \"\"\"\n",
    "    Splits matching pairs of winodws and labels into train and test splits.\n",
    "    \"\"\"\n",
    "    split_size = int(len(windows)* (1-test_split)) # this will default to 80% train/20% test\n",
    "    train_windows = windows[:split_size]\n",
    "    train_labels = labels[:split_size]\n",
    "    test_windows = windows[split_size:]\n",
    "    test_labels = labels[split_size:]\n",
    "    return train_windows, test_windows, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test windows\n",
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows,\n",
    "                                                                                labels=full_labels)\n",
    "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c281d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows[:5], train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72410c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_windows[:5], test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if train labels are the same (before and after window split)\n",
    "np.array_equal(np.squeeze(train_labels[:-HORIZON-1]),\n",
    "               y_train[WINDOW_SIZE:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9969c83",
   "metadata": {},
   "source": [
    "Video N°316: Creating a modelling checkpoint to save our best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163648f2",
   "metadata": {},
   "source": [
    "## Make a modelling checkpoint callback\n",
    "\n",
    "Because our model's performance will fluctuate from experiment to experiment, we're going to write a model checkpoint so we can compare apples to apples.\n",
    "\n",
    "More specifically, we want to compare each of our model's best performances against the other model's best performances.\n",
    "\n",
    "For example if our model performs the best on epoch 55 (but we're training for 100 epochs), we want to load and evaluate the model saved on epoch 55.\n",
    "\n",
    "We can create a modelling checkpoint callback using the following: [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bbfb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to implement a ModelCheckpoint callback with a specific filename\n",
    "# already exists in my personal library\n",
    "from MachineLearningUtils.training_utilities.model_callbacks import create_model_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d9205",
   "metadata": {},
   "source": [
    "Video N°317: Model 1:Building, compiling and fitting a deep learning model on Bitcoin data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53db9c",
   "metadata": {},
   "source": [
    "## Model 1: Dense model (window = 7, horizon = 1)\n",
    "\n",
    "Our first deep model is going to be a simple dense model:\n",
    "* A single dense layer with 128 hidden units and ReLU\n",
    "* An output layer with linear activation (no activation)\n",
    "* Adam optimizaiton and MAE loss function\n",
    "* Batch size of 128 (previously we've used 32) \n",
    "* 100 epochs \n",
    "\n",
    "Why these values?\n",
    "\n",
    "I picked them out of experimentation.\n",
    "\n",
    "* **Hyperparameters** = values a machine learning practitioner (you!) can adjust themselves\n",
    "* **Parameters** = values a model learns on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set random seed for as reproducible results as possible\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Construct model\n",
    "model_1 = tf.keras.Sequential([\n",
    "    layers.Dense(units=128, activation=\"relu\"),\n",
    "    layers.Dense(units=HORIZON, activation=\"linear\") # linear activation is the same as having no activation\n",
    "], name=\"model_1_dense\") # name our model so we can save it\n",
    "\n",
    "# 2. Compile\n",
    "model_1.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "# 3. Fit the model\n",
    "model_1.fit(x=train_windows,\n",
    "            y=train_labels,\n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_1.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "model_1.evaluate(x=test_windows, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ea0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in saved best performing model_1 and evaluate it on test data\n",
    "model_1 = tf.keras.models.load_model(filepath=\"model_experiments/model_1_dense/\")\n",
    "model_1.evaluate(x=test_windows, y=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b34fd",
   "metadata": {},
   "source": [
    "Video N°318: Creating a function to make predictions with our trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2b86d",
   "metadata": {},
   "source": [
    "## Making forecasts with a model (on the test dataset)\n",
    "\n",
    "To make \"forecasts\" on the test dataset (note: these won't be actual forecasts, they're only psuedo forecasts because actual forecasts are into the future), let's write a function to:\n",
    "\n",
    "1. Take in a train model\n",
    "2. Takes in some input data (same kind of data the model was trained on)\n",
    "3. Passes the input data to the model's `predict()` method\n",
    "4. Returns the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decfe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(model, input_data):\n",
    "    \"\"\"\n",
    "    Uses model to m\n",
    "    ake predictions input_data.\n",
    "    \"\"\"\n",
    "    forecast = model.predict(x=input_data)\n",
    "    return tf.squeeze(forecast) # return 1D array of predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d257012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using model_1 on the test dataset and view results\n",
    "model_1_preds = make_preds(model=model_1, input_data=test_windows)\n",
    "len(model_1_preds), model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate preds\n",
    "model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e62c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aba737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot our model 1 predictions\n",
    "offset = 450\n",
    "plt.figure(figsize=(20,14))\n",
    "# Account for the test_window offset and index into test_labels to ensure correct plotting\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):],\n",
    "                 values=test_labels[:, 0],\n",
    "                 start=offset,\n",
    "                 label=\"Test Data\")\n",
    "\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):],\n",
    "                 values=model_1_preds,\n",
    "                 start=offset,\n",
    "                 format=\"-\",\n",
    "                 label=\"model_1_preds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194cae6",
   "metadata": {},
   "source": [
    "Video N°319: Model 2: Building, fitting and evaluating a deep model with a larger window size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f0844",
   "metadata": {},
   "source": [
    "## Model 2: Dense (window = 30, horizon = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dba5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 1 # predict one step at a time (one day of Bitcoin prices)\n",
    "WINDOW_SIZE = 30 # use 30 timesteps in past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01135f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make windowed data with appropriate horizon and window sizes\n",
    "full_windows, full_labels = make_windows(x=prices,\n",
    "                                         window_size=WINDOW_SIZE,\n",
    "                                         horizon=HORIZON)\n",
    "len(full_windows), len(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da695fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and testing windows\n",
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows,\n",
    "                                                                                labels=full_labels,\n",
    "                                                                                test_split=0.2)\n",
    "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_2 = tf.keras.Sequential([\n",
    "    layers.Dense(units=128, activation=\"relu\"),\n",
    "    layers.Dense(units=HORIZON, activation=\"linear\")\n",
    "], name=\"model_2_dense\")\n",
    "\n",
    "# Compile\n",
    "model_2.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Fit\n",
    "model_2.fit(x=train_windows,\n",
    "            y=train_labels,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            verbose=0,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_2.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d401636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model 2 on test data\n",
    "model_2.evaluate(x=test_windows, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91fd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the best performing model\n",
    "model_2 = tf.keras.models.load_model(filepath=\"model_experiments/model_2_dense/\")\n",
    "model_2.evaluate(x=test_windows, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc30c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get forecast predictions\n",
    "model_2_preds = make_preds(model=model_2, input_data=test_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(test_labels).shape, model_2_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4526ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results for model_2 predictions\n",
    "model_2_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca6a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset=300\n",
    "plt.figure(figsize=(20, 14))\n",
    "# Account for test_window offset when plotting\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):],\n",
    "                 values=test_labels[:, 0],\n",
    "                 start=offset,\n",
    "                 format=\"-\",\n",
    "                 label=\"Test Data\")\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):],\n",
    "                 values=model_2_preds,\n",
    "                 start=offset,\n",
    "                 format='-',\n",
    "                 label=\"model_2_preds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed5c4f",
   "metadata": {},
   "source": [
    "Video N°320: Model 3: Building, fitting and evaluating a model with larger horizon size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec45b05",
   "metadata": {},
   "source": [
    "## Model 3: Dense (window = 30, horizon = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc995317",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e77762",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 7\n",
    "WINDOW_SIZE = 30\n",
    "\n",
    "full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\n",
    "len(full_windows), len(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows,\n",
    "                                                                                labels=full_labels)\n",
    "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create model (same as model_1 except with different data input and output sizes)\n",
    "model_3 = tf.keras.Sequential([\n",
    "    layers.Dense(units=128, activation=\"relu\"),\n",
    "    layers.Dense(units=HORIZON)\n",
    "], name=\"model_3_dense\")\n",
    "\n",
    "# Compile\n",
    "model_3.compile(loss=\"MAE\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Fit\n",
    "model_3.fit(x=train_windows,\n",
    "            y=train_labels,\n",
    "            batch_size=128,\n",
    "            epochs=100,\n",
    "            verbose=0,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_3.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8353e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "model_3.evaluate(x=test_windows, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482f9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best version of model_3 and evaluate\n",
    "model_3 = tf.keras.models.load_model(filepath=\"model_experiments/model_3_dense\")\n",
    "model_3.evaluate(x=test_windows, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0139705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model_3\n",
    "model_3_preds = make_preds(model=model_3,\n",
    "                           input_data=test_windows)\n",
    "model_3_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3_preds.shape, model_2_preds.shape, model_1_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecc1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model_3 results (these are going multi-dimensional because we're trying to predict more than one timestep at a time)\n",
    "model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c091e6",
   "metadata": {},
   "source": [
    "Video N°321: Adjusting the evaluation function to work with larger horizons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bdca4",
   "metadata": {},
   "source": [
    "## Make our evaluation function work for larger horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52966731",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(input_tensor=model_3_results[\"mae\"]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e68b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_results[\"mae\"].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5644bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to take in model predictions and truth values and return evaluation metrics\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "  # Make sure float32 datatype (for metric calculations)\n",
    "  y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "  y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "  # Calculate various evaluation metrics \n",
    "  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "  rmse = tf.sqrt(mse)\n",
    "  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "  mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "\n",
    "  # Account for different sized metrics (for longer horizons, we want to reduce metrics to a single value)\n",
    "  if mae.ndim > 0:\n",
    "    mae = tf.reduce_mean(mae)\n",
    "    mse = tf.reduce_mean(mse)\n",
    "    rmse = tf.reduce_mean(rmse)\n",
    "    mape = tf.reduce_mean(mape)\n",
    "    mase = tf.reduce_mean(mase)\n",
    "\n",
    "  return {\"mae\": mae.numpy(),\n",
    "          \"mse\": mse.numpy(),\n",
    "          \"rmse\": rmse.numpy(),\n",
    "          \"mape\": mape.numpy(),\n",
    "          \"mase\": mase.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model_3 results aggregated to single values\n",
    "model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f12193",
   "metadata": {},
   "source": [
    "Video N°322: Model 3: Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d8cf6",
   "metadata": {},
   "source": [
    "We've made and evaluated predictions with `model_3`, let's visualize, visualize, visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe421db",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 300\n",
    "plt.figure(figsize=(20, 14))\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):],\n",
    "                 values=test_labels[:, 0],\n",
    "                 start=offset,\n",
    "                 format='-',\n",
    "                 label='Test data')\n",
    "plot_time_series(timesteps=X_test[-len(test_windows):],\n",
    "                 values=tf.reduce_mean(model_3_preds, axis=1),\n",
    "                 start=offset,\n",
    "                 label=\"model_3_preds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bceaa98",
   "metadata": {},
   "source": [
    "Video N°323: Comparing our modelling experiments so far and discussing autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edef1f9",
   "metadata": {},
   "source": [
    "## Which of our models is performing the best so far? \n",
    "\n",
    "So we've trained a few models, now let's compare them and see how they've gone... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"naive\": naive_results[\"mae\"],\n",
    "              \"horizon_1_window_7\": model_1_results[\"mae\"],\n",
    "              \"horizon_1_window_30\": model_2_results[\"mae\"],\n",
    "              \"horizon_7_window_30\": model_3_results[\"mae\"]},\n",
    "             index=[\"mae\"]).plot(figsize=(20, 14), kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221fe19",
   "metadata": {},
   "source": [
    "> 🔑 **Note:** You might be wondering, why are the naïve results so good? One of the reasons could be due the presence of autocorrelation in the data. If a time series has autocorrelation it means the value at `t+1` (the next timestep) is typically close to the value at `t` (the current timestep). In other words, today's value is probably pretty close to yesterday's value. Of course, this isn't always the case but when it is (stock market prices often seem autocorrelated but often fluctuate randomly), a naïve model will often get fairly good results.\n",
    "\n",
    "> 📖 **Resource:** For more on how autocorrelation influences a model's predictions, see the article [How (not) to use Machine Learning for time series forecasting: Avoiding the pitfalls by Vegard Flovik](https://towardsdatascience.com/how-not-to-use-machine-learning-for-time-series-forecasting-avoiding-the-pitfalls-19f9d7adf424)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c860ba5",
   "metadata": {},
   "source": [
    "Video N°324: Preparing data for building a Conv1D model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5e891",
   "metadata": {},
   "source": [
    "## Model 4: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7921f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 1 # predict next day of Bitcoin prices\n",
    "WINDOW_SIZE = 7  # use previous week worth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windowed data\n",
    "full_windows, full_labels = make_windows(x=prices, window_size=WINDOW_SIZE, horizon=HORIZON)\n",
    "len(full_windows), len(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test sets\n",
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows,\n",
    "                                                                                labels=full_labels)\n",
    "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20218556",
   "metadata": {},
   "source": [
    "To use the Conv1D layer, we need an input shape of: `(batch_size, timesteps, input_dim)`...\n",
    "\n",
    "However our data isn't in that shape yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05848248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data input shape\n",
    "train_windows[0].shape # returns (WINDOW_SIZE, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ee51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we pass our data to the Conv1D layer, we have to reshape it in order to make sure it works\n",
    "x = tf.constant(train_windows[0])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1)) # add an extra dimension for `input_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de24501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out our lambda layer\n",
    "print(f\"Original shape: {x.shape}\") # (WINDOW_SIZE)\n",
    "print(f\"Expanded shape: {expand_dims_layer(x).shape}\") # output = ...?\n",
    "print(f\"Original values with expanded shape:\\n {expand_dims_layer(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d161a",
   "metadata": {},
   "source": [
    "Video N°325: Model 4: Building, fitting and evaluating a Conv1D model on our Bitcoin data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b367d2",
   "metadata": {},
   "source": [
    "[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)\n",
    "`padding=\"causal\"`:\n",
    "\"causal\" results in causal(dilated) convolutions, e.g. output[t] does not depend oninput[t+1:]. Useful when modeling temporal data where the model should not violate the temporal order. See WaveNet: A Generative Model for Raw Audio, section2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create Conv1D model\n",
    "model_4 = tf.keras.Sequential([\n",
    "    layers.Lambda(lambda x: tf.expand_dims(x, axis=1)),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    layers.Dense(units=HORIZON)\n",
    "], name=\"model_4_conv1D\")\n",
    "\n",
    "# Compile model\n",
    "model_4.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Fit\n",
    "model_4.fit(x=train_windows,\n",
    "            y=train_labels,\n",
    "            batch_size=128,\n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_4.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model...\n",
    "model_4.evaluate(x=test_windows, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back in best performing Conv1D and re-evaluate\n",
    "model_4 = tf.keras.models.load_model(filepath=\"model_experiments/model_4_conv1D\")\n",
    "model_4.evaluate(x=test_windows, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model_4_preds = make_preds(model=model_4, input_data=test_windows)\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "model_4_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26859c",
   "metadata": {},
   "source": [
    "Video N°326: Model 5: Building, fitting and evaluating a LSTM (RNN) model on our Bitcoin data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ed3b7",
   "metadata": {},
   "source": [
    "## Model 5: RNN (LSTM)\n",
    "\n",
    "Let's build an RNN model for our time series data.\n",
    "\n",
    "We'll use the same data we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745096aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Let's build an LSTM model with the Functional API\n",
    "inputs = layers.Input(shape=(WINDOW_SIZE))\n",
    "x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs)\n",
    "# x = layers.LSTM(units=128, return_sequences=True)(x)\n",
    "x = layers.LSTM(units=128, activation=\"relu\")(x)\n",
    "# x = layers.Dense(units=32, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(units=HORIZON)(x)\n",
    "model_5 = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"model_5_LSTM\")\n",
    "\n",
    "# Compile\n",
    "model_5.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Fit\n",
    "model_5.fit(x=train_windows,\n",
    "            y=train_labels,\n",
    "            epochs=100,\n",
    "            verbose=1,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_5.name)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in best version of model 5 and evaluate on the test data\n",
    "model_5 = tf.keras.models.load_model(\"model_experiments/model_5_LSTM\")\n",
    "model_5.evaluate(x=test_windows, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction with our LSTM model\n",
    "model_5_preds = make_preds(model=model_5, input_data=test_windows)\n",
    "model_5_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model 5 predictions\n",
    "model_5_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_5_preds)\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fed36f",
   "metadata": {},
   "source": [
    "> 🔑 **Note:** Because neural networks are such powerful algorithms, they can be used for almost any problem, however, that doesn't mean they'll achieve performant or usable results. You're probably starting to clue onto this now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9edcd3",
   "metadata": {},
   "source": [
    "Video N°327: Investigating how to turn our univariate time series into multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3625c21",
   "metadata": {},
   "source": [
    "## Make a multivariate time series dataset\n",
    "\n",
    "Before we add a feature to our time series, what is a feature we can use?\n",
    "\n",
    "One feature we could add is whether or not Daniel Bourke tweeted on that day...\n",
    "\n",
    "* day 1 - 0\n",
    "* day 2 - 1\n",
    "* day 3 - 0\n",
    "\n",
    "What's a better feature to use?\n",
    "\n",
    "How about the bitcoin halving events?\n",
    "\n",
    "[https://www.cmcmarkets.com/en/learn-cryptocurrencies/bitcoin-halving](https://www.cmcmarkets.com/en/learn-cryptocurrencies/bitcoin-halving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf13e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a multivariate time series\n",
    "bitcoin_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557010ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the bitcoin halving events to our dataset\n",
    "block_reward_1 = 50 # 3 January 2009 - this block reward isn't in our dataset (our data starts from 01 October 2013)\n",
    "block_reward_2 = 25 # 8 November 2012\n",
    "block_reward_3 = 12.5 # 9 July 2016\n",
    "block_reward_4 = 6.25 # 18 May 2020\n",
    "\n",
    "# Block reward dates\n",
    "block_reward_2_datetime = np.datetime64(\"2012-11-28\")\n",
    "block_reward_3_datetime = np.datetime64(\"2016-07-09\")\n",
    "block_reward_4_datetime = np.datetime64(\"2020-05-18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91adae8e",
   "metadata": {},
   "source": [
    "Video N°328: Creating and plotting a multivariate time series with BTC price and block reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64aa7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date ranges of where specific block_reward values should be\n",
    "block_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days\n",
    "block_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days\n",
    "block_reward_2_days, block_reward_3_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf866961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in a block_reward column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84181caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_prices_block = bitcoin_prices.copy()\n",
    "bitcoin_prices_block[\"block_reward\"] = None\n",
    "\n",
    "# Add in block_reward values as a feature to our dataframe\n",
    "bitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2\n",
    "bitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3\n",
    "bitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_prices_block.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4acbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the block reward vs price over time\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "scaled_price_block_df = pd.DataFrame(data=minmax_scale(X=bitcoin_prices_block[[\"Price\", \"block_reward\"]]),\n",
    "                                     columns=bitcoin_prices_block.columns,\n",
    "                                     index=bitcoin_prices_block.index)\n",
    "scaled_price_block_df.plot(figsize=(20, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ede8ad",
   "metadata": {},
   "source": [
    "Video N°329: Preparing our multivariate time series for a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e3f57",
   "metadata": {},
   "source": [
    "## Making a windowed dataset with pandas\n",
    "\n",
    "Previously, we've turned our univariate time series into windowed dataset using the helper functions above.\n",
    "\n",
    "However, since we've got multivariate data, these functions won't work.\n",
    "\n",
    "Not to worry, we can use the pandas.DataFrame.shift() method to window our multivariate data.\n",
    "\n",
    "[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae581bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset hyperparameters\n",
    "HORIZON = 1\n",
    "WINDOW_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa54ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the Bitcoin historical data with block reward feature\n",
    "bitcoin_prices_windowed = bitcoin_prices_block.copy()\n",
    "\n",
    "# Add windowed columns\n",
    "for i in range(WINDOW_SIZE): # shift values for each step in WINDOW_SIZE\n",
    "    bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1)\n",
    "bitcoin_prices_windowed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a43144",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_prices_windowed.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906fa854",
   "metadata": {},
   "source": [
    "What we've done is created a way to window our dataset directly from a pandas dataframe.\n",
    "\n",
    "```\n",
    "[0, 1, 2, 3, 4, 5, 6, block_reward] -> [7]\n",
    "[1, 2, 3, 4, 5, 6, 7, block_reward] -> [8]\n",
    "[2, 3, 4, 5, 6, 7, 8, block_reward] -> [9]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X (windows) and y (horizons) features\n",
    "X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32)\n",
    "y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb87f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and test sets using indexing\n",
    "split_size = int(len(X) * 0.8)\n",
    "X_train, y_train = X[:split_size], y[:split_size]\n",
    "X_test, y_test = X[split_size:], y[split_size:]\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63da641",
   "metadata": {},
   "source": [
    "Video N°330: Model 6: Building, fitting and evaluating a multivariate time series model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d3d35",
   "metadata": {},
   "source": [
    "## Model 6: Dense (multivariate time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Make multivariate time series model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a049b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d243046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
